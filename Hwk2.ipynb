{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation du reseau de neurones et experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Implémentation de fonctions utiles\n",
    "Calcul numériquement stable du softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_vector(x):\n",
    "    '''\n",
    "    x:  is a data vector.\n",
    "    returns: the result of the softmax function applied to the data vector.\n",
    "    '''\n",
    "    max_comp = np.amax(x)\n",
    "    normalized  = x - max_comp\n",
    "    \n",
    "    exponential = np.exp(normalized)\n",
    "    \n",
    "    return exponential/np.sum(exponential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    '''\n",
    "    X: matrix that holds the data, every row is a data vector.\n",
    "    returns: matrix where every row is the result of the softmax function applied to the corresponding data vector.\n",
    "    '''\n",
    "    \n",
    "    max_comp = np.amax(X, axis=1)\n",
    "    normalized  = X - max_comp.reshape(X.shape[0], 1)\n",
    "    \n",
    "    exponential = np.exp(normalized)\n",
    "    \n",
    "    return exponential/np.sum(exponential, axis=1).reshape(X.shape[0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction utilitaire pour calculer relu($x$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x, np.zeros(x.shape))\n",
    "\n",
    "def onehot(m, y):\n",
    "    return np.eye(m)[y]\n",
    "\n",
    "def onehot_matrix(m, targets):\n",
    "    \"\"\"\n",
    "    Returns: onehot matrix where every column is a onehot vector of the coressponding target\n",
    "    \"\"\"\n",
    "    eye = np.eye(m)\n",
    "    onehot_matrix = np.zeros((m,len(targets)))\n",
    "    \n",
    "    for i, y in enumerate(targets):\n",
    "        onehot_matrix[:,i] = eye[y]\n",
    "        \n",
    "    return onehot_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Calcul du gradient sur un exemple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation de fprop et bprop pour calculer le gradient sur un exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    \n",
    "    def __init__(self, n_input, n_hidden, n_out):\n",
    "        \n",
    "        self.n_in = n_input\n",
    "        self.n_h = n_hidden\n",
    "        self.n_o = n_out\n",
    "        \n",
    "        low_bound = -1 / np.sqrt([self.n_in, self.n_h])\n",
    "        up_bound = 1 / np.sqrt([self.n_in, self.n_h])\n",
    "        \n",
    "        # Initialize the parameters\n",
    "        self.W1 = np.random.uniform(low_bound[0], up_bound[0], size=(self.n_h, self.n_in))  # d_h x d\n",
    "        self.W2 = np.random.uniform(low_bound[1], up_bound[1], size=(self.n_o, self.n_h))  # m x d_h\n",
    "        self.b1 = np.zeros(self.n_h)  # dimension d_h\n",
    "        self.b2 = np.zeros(self.n_o) # dimension m\n",
    "    \n",
    "    def fprop(self, x):\n",
    "        '''Computes activations for every layer'''\n",
    "        self.ha = self.W1.dot(x) + self.b1\n",
    "        self.hs = relu(self.ha)\n",
    "        self.oa = self.W2.dot(self.hs) + self.b2\n",
    "        self.os = softmax_vector(self.oa)\n",
    "            \n",
    "    def bprop(self, x, y):\n",
    "        '''Computes the gradients, must be executed after fprop'''\n",
    "                      \n",
    "        grad_oa = self.os - onehot(self.n_o, y)\n",
    "        grad_b2 = grad_oa\n",
    "        grad_W2 = np.outer(grad_oa, self.hs)\n",
    "        grad_hs = self.W2.T.dot(grad_oa)\n",
    "        grad_ha = grad_hs * (self.ha > 0)\n",
    "        grad_W1 = np.outer(grad_ha, x)\n",
    "        grad_b1 = grad_ha\n",
    "        \n",
    "        return grad_W1, grad_W2, grad_b1, grad_b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vérification du gradient par différences finies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction pour calculer les differences finies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(x, y, W1, W2, b1, b2):\n",
    "    ha = W1.dot(x) + b1\n",
    "    hs = relu(ha)\n",
    "    oa = W2.dot(hs) + b2\n",
    "    os = softmax_vector(oa)\n",
    "    \n",
    "    return -np.log(os[y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant vérifier que le gradient calculé par Keras est juste. Pour cela on va calculer une approximation du gradient en utilisant la méthode de la différence finie.\n",
    "\n",
    "Pour chaque composante de w1, on va:\n",
    " 1. calculer la valeur de la fonction objectif $L$\n",
    " 2. ajouter une petite valeur $\\epsilon$ à la composante\n",
    " 3. recalculer la valeur de la fonction objectif $L'$\n",
    " 4. remettre à l'ancienne valeur la composante (c'est à dire soustraire $\\epsilon$)\n",
    "\n",
    "Le gradient par différences finies sera dans ce cas donné par:\n",
    "$\\Big( \\frac{\\partial L}{\\partial W_{1}} \\Big)_{ij} = \\frac{1}{\\epsilon} (L' - L)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def finite_diff(x, y, neural_net, eps=1e-5):\n",
    "    \n",
    "    # params\n",
    "    W1 = neural_net.W1\n",
    "    W2 = neural_net.W2\n",
    "    b1 = neural_net.b1\n",
    "    b2 = neural_net.b2\n",
    "    \n",
    "    # gradients\n",
    "    grad_w1_diff = np.zeros(W1.shape)\n",
    "    grad_w2_diff = np.zeros(W2.shape)\n",
    "    grad_b1_diff = np.zeros(b1.shape)\n",
    "    grad_b2_diff = np.zeros(b2.shape)\n",
    "    \n",
    "    neural_net.fprop(x)\n",
    "    loss = compute_loss(x, y, W1, W2, b1, b2)\n",
    "    \n",
    "    for i in range(W1.shape[0]):\n",
    "        for j in range(W1.shape[1]):\n",
    "            W1[i,j] = W1[i,j] + eps\n",
    "            loss_prime = compute_loss(x, y, W1, W2, b1, b2)\n",
    "            grad_w1_diff[i, j] = (loss_prime - loss) / epsilon\n",
    "            W1[i,j] = W1[i,j] - eps\n",
    "    for i in range(W2.shape[0]):\n",
    "        for j in range(W2.shape[1]):\n",
    "            W2[i,j] = W2[i,j] + eps\n",
    "            loss_prime = compute_loss(x, y, W1, W2, b1, b2)\n",
    "            grad_w2_diff[i, j] = (loss_prime - loss) / epsilon\n",
    "            W2[i,j] = W2[i,j] - eps\n",
    "    for i in range(b1.shape[0]):\n",
    "            b1[i] = b1[i] + eps\n",
    "            loss_prime = compute_loss(x, y, W1, W2, b1, b2)\n",
    "            grad_b1_diff[i] = (loss_prime - loss) / epsilon\n",
    "            b1[i] = b1[i] - eps\n",
    "    for i in range(b1.shape[0]):\n",
    "            b2[i] = b2[i] + eps\n",
    "            loss_prime = compute_loss(x, y, W1, W2, b1, b2)\n",
    "            grad_b2_diff[i] = (loss_prime - loss) / epsilon\n",
    "            b2[i] = b2[i] - eps\n",
    "            \n",
    "    return grad_w1_diff, grad_w2_diff, grad_b1_diff, grad_b2_diff\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt(open('2moons.txt','r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage de vérification du gradient par différence finie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.07384288  0.36316756]\n",
      " [-0.03826933  0.1881425 ]]\n",
      "[[-0.07385257  0.3629334 ]\n",
      " [-0.03827194  0.18807962]]\n",
      "[[ -9.69255406e-06  -2.34165197e-04]\n",
      " [ -2.60304101e-06  -6.28763501e-05]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zimmae\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "C:\\Users\\zimmae\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNet(2, 2, 2)\n",
    "x = data[0,:-1]\n",
    "y = data[0,-1]\n",
    "epsilon = 1e-3\n",
    "\n",
    "grad_w1_diff, grad_w2_diff, grad_b1_diff, grad_b2_diff = finite_diff(x, y, nn, epsilon)\n",
    "grad_W1, grad_W2, grad_b1, grad_b2 = nn.bprop(x,y)\n",
    "\n",
    "print(grad_w1_diff)\n",
    "print(grad_W1)\n",
    "print(grad_W1 - grad_w1_diff)\n",
    "#print(grad_W2 - grad_w2_diff)\n",
    "#print(grad_b1 - grad_b1_diff)\n",
    "#print(grad_b2 - grad_b2_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 et 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    \n",
    "    def __init__(self, n_input, n_hidden, n_out, lambdas):\n",
    "        \n",
    "        self.n_in = n_input\n",
    "        self.n_h = n_hidden\n",
    "        self.n_o = n_out\n",
    "        self.lambdas = lambdas\n",
    "        \n",
    "        low_bound = -1 / np.sqrt([self.n_in, self.n_h])\n",
    "        up_bound = 1 / np.sqrt([self.n_in, self.n_h])\n",
    "        \n",
    "        # Initialize the parameters\n",
    "        self.W1 = np.random.uniform(low_bound[0], up_bound[0], size=(self.n_h, self.n_in))  # d_h x d\n",
    "        self.W2 = np.random.uniform(low_bound[1], up_bound[1], size=(self.n_o, self.n_h))  # m x d_h\n",
    "        self.b1 = np.zeros(self.n_h)  # dimension d_h\n",
    "        self.b2 = np.zeros(self.n_o) # dimension m\n",
    "    \n",
    "    def fprop(self, x):\n",
    "        '''Computes activations for every layer'''\n",
    "        self.ha = self.W1.dot(x) + self.b1\n",
    "        self.hs = relu(ha)\n",
    "        self.oa = self.W2.dot(hs) + self.b2\n",
    "        self.os = softmax_vector(oa)\n",
    "            \n",
    "    def bprop(self, x, y):\n",
    "        '''Computes the gradients, must be executed after fprop'''\n",
    "                      \n",
    "        grad_oa = os - onehot(self.n_out, y)\n",
    "        grad_b2 = grad_oa\n",
    "        grad_W2 = np.outer(grad_oa, self.hs)\n",
    "        grad_hs = self.W2.T.dot(grad_oa)\n",
    "        grad_ha = grad_hs * (self.ha > 0)\n",
    "        grad_W1 = np.outer(grad_ha, x)\n",
    "        grad_b1 = grad_ha\n",
    "        \n",
    "        return grad_W1, grad_W2, grad_b1, grad_b2\n",
    "    \n",
    "    def compute_loss(self, y):\n",
    "        return -np.log(self.os[y])\n",
    "    \n",
    "    def train(self, train_data, max_iter, batch_size, eta=0.05):\n",
    "        \n",
    "        n_batches = np.ceil(train_data.shape[0]/batch_size) # number of batches\n",
    "        \n",
    "        # Initialize batch start and end indices\n",
    "        batch_start = 0\n",
    "        if (batch_start + batch_size < train_data.shape[0]):\n",
    "            batch_end = batch_start + batch_size\n",
    "        else:\n",
    "            batch_end = train_data.shape[0]\n",
    "        \n",
    "        for i in range(max_iter):\n",
    "            for j in range(n_batches):\n",
    "                \n",
    "                batch = train_data[batch_start:batch_end]\n",
    "                grad_W1_mean = np.zeros((self.n_h, self.n_in))\n",
    "                grad_W2_mean = np.zeros((self.n_o, self.n_h))\n",
    "                grad_b1_mean = np.zeros(self.n_h)\n",
    "                grad_b2_mean = np.zeros(self.n_o)\n",
    "                \n",
    "                for elem in batch:\n",
    "                    self.fprop(elem[-1])                    \n",
    "                    losses.push(compute_loss(elem[-1]))\n",
    "                    \n",
    "                    grad_W1, grad_W2, grad_b1, grad_b2 = self.bprop(elem[:-1], elem[-1])\n",
    "                    \n",
    "                    grad_w1_mean = grad_W1_mean + grad_W1\n",
    "                    grad_w2_mean = grad_W2_mean + grad_W2\n",
    "                    grad_b1_mean = grad_b1_mean + grad_b1\n",
    "                    grad_b2_mean = grad_b2_mean + grad_b2\n",
    "                \n",
    "                # Compute mean of loss/cost over all examples in the batch\n",
    "                n = len(batch)\n",
    "                grad_w1_mean = grad_W1_mean / n\n",
    "                grad_w2_mean = grad_W2_mean / n\n",
    "                grad_b1_mean = grad_b1_mean / n\n",
    "                grad_b2_mean = grad_b2_mean / n\n",
    "                \n",
    "                #regularization\n",
    "                penality_grad_W1 = self.lambdas[0][0] * np.sign(self.W1) + 2 * self.lambdas[0][1] * self.W1\n",
    "                penality_grad_W2 = self.lambdas[1][0] * np.sign(self.W2) + 2 * self.lambdas[1][1] * self.W2\n",
    "                \n",
    "                self.W1 = self.W1 - eta * (grad_W1_mean + penality_grad_W1)\n",
    "                self.W2 = self.W2 - eta * (grad_W2_mean + penality_grad_W2)\n",
    "                self.b1 = self.b1 - eta * grad_b1_mean\n",
    "                self.b2 = self.b2 - eta * grad_b2_mean\n",
    "                \n",
    "                # Get next batch\n",
    "                batch_start = batch_end + 1        \n",
    "                if (batch_start + batch_size < train_data.shape[0]):\n",
    "                    batch_end = batch_start + batch_size\n",
    "                else:\n",
    "                    batch_end = train_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Entrainement sur 2 moons et visualisation des regions de decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: Optimisation du calcule de gradient pour mini-batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{W}^{(1)} \\in \\mathbb{R}^{d_h \\times d}$, $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ et $\\mathbf{B}^{(1)} \\in \\mathbb{R}^{d_h \\times n}$\n",
    "\n",
    "$$\\mathbf{h}^{a} = \\mathbf{W}^{(1)}\\mathbf{X}^{\\top} + \\mathbf{B}^{(1)} \\in \\mathbb{R}^{d_h \\times n}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetVectorized:\n",
    "    \n",
    "    def __init__(self, n_input, n_hidden, n_out, lambdas):\n",
    "        \n",
    "        self.n_in = n_input\n",
    "        self.n_h = n_hidden\n",
    "        self.n_o = n_out\n",
    "        self.lambdas = lambdas\n",
    "        \n",
    "        low_bound = -1 / np.sqrt([self.n_in, self.n_h])\n",
    "        up_bound = 1 / np.sqrt([self.n_in, self.n_h])\n",
    "        \n",
    "        # Initialize the parameters\n",
    "        self.W1 = np.random.uniform(low_bound[0], up_bound[0], size=(self.n_h, self.n_in))  # d_h x d\n",
    "        self.W2 = np.random.uniform(low_bound[1], up_bound[1], size=(self.n_o, self.n_h))  # m x d_h\n",
    "        self.b1 = np.zeros(self.n_h)  # dimension d_h\n",
    "        self.b2 = np.zeros(self.n_o) # dimension m\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        '''\n",
    "        Computes activations for every layer\n",
    "        X: input data set\n",
    "        '''\n",
    "        self.ha = self.W1.dot(X.T) + self.b1.reshape(self.n_h, 1)\n",
    "        self.hs = relu(ha)\n",
    "        self.oa = self.W2.dot(hs) + self.b2.reshape(self.n_o, 1)\n",
    "        self.os = softmax(oa)\n",
    "            \n",
    "    def bprop(self, X, Y):\n",
    "        '''\n",
    "        Computes the gradients, must be executed after fprop\n",
    "        X: Input data set\n",
    "        Y: targets\n",
    "        '''\n",
    "                      \n",
    "        grad_oa = os - onehot_matrix(self.n_out, Y)\n",
    "        grad_b2 = grad_oa # m x n\n",
    "        grad_W2 = np.dot(grad_oa, self.hs.T) # sum of gradients grad_W2 for each example\n",
    "        grad_hs = self.W2.T.dot(grad_oa) # d_h x n\n",
    "        grad_ha = grad_hs * (self.ha > 0) # d_h x n\n",
    "        grad_W1 = np.dot(grad_ha, X.T) # sum of gradients grad_W1 for each example\n",
    "        grad_b1 = grad_ha # d_h x n\n",
    "        \n",
    "        return grad_W1, grad_W2, grad_b1, grad_b2\n",
    "    \n",
    "    def compute_loss(self, y):\n",
    "        # TODO: adjust for y vector\n",
    "        return -np.log(self.os)\n",
    "    \n",
    "    def train(self, train_data, max_iter, batch_size, eta=0.05):\n",
    "        \n",
    "        n_batches = np.ceil(train_data.shape[0]/batch_size) # number of batches\n",
    "        \n",
    "        # Initialize batch start and end indices\n",
    "        batch_start = 0\n",
    "        if (batch_start + batch_size < train_data.shape[0]):\n",
    "            batch_end = batch_start + batch_size\n",
    "        else:\n",
    "            batch_end = train_data.shape[0]\n",
    "        \n",
    "        for i in range(max_iter):\n",
    "            for j in range(n_batches):\n",
    "                \n",
    "                batch = train_data[batch_start:batch_end]\n",
    "                \n",
    "                self.fprop(batch[:,:-1])\n",
    "                grad_W1, grad_W2, grad_b1, grad_b2 = self.bprop(batch[:,:-1], batch[:,-1]) \n",
    "                \n",
    "                n = len(batch)\n",
    "                \n",
    "                #regularization\n",
    "                penality_grad_W1 = self.lambdas[0][0] * np.sign(self.W1) + 2 * self.lambdas[0][1] * self.W1\n",
    "                penality_grad_W2 = self.lambdas[1][0] * np.sign(self.W2) + 2 * self.lambdas[1][1] * self.W2\n",
    "                \n",
    "                self.W1 = self.W1 - eta * ((grad_W1 / n) + penality_grad_W1)\n",
    "                self.W2 = self.W2 - eta * ((grad_W2 / n) + penality_grad_W2)\n",
    "                self.b1 = self.b1 - eta * np.mean(grad_b1, axis=1)\n",
    "                self.b2 = self.b2 - eta * np.mean(grad_b2, axis=1)\n",
    "                \n",
    "                \n",
    "                # Get next batch\n",
    "                batch_start = batch_end + 1        \n",
    "                if (batch_start + batch_size < train_data.shape[0]):\n",
    "                    batch_end = batch_start + batch_size\n",
    "                else:\n",
    "                    batch_end = train_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
