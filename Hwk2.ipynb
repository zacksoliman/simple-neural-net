{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation du reseau de neurones et experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Implémentation de fonctions utiles\n",
    "Calcul numériquement stable du softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_vector(x):\n",
    "    '''\n",
    "    x:  is a data vector.\n",
    "    returns: the result of the softmax function applied to the data vector.\n",
    "    '''\n",
    "    max_comp = np.amax(x)\n",
    "    normalized  = x - max_comp\n",
    "    \n",
    "    exponential = np.exp(normalized)\n",
    "    \n",
    "    return exponential/np.sum(exponential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    '''\n",
    "    X: matrix that holds the data, every row is a data vector.\n",
    "    returns: matrix where every row is the result of the softmax function applied to the corresponding data vector.\n",
    "    '''\n",
    "    \n",
    "    max_comp = np.amax(X, axis=1)\n",
    "    normalized  = X - max_comp.reshape(X.shape[0], 1)\n",
    "    \n",
    "    exponential = np.exp(normalized)\n",
    "    \n",
    "    return exponential/np.sum(exponential, axis=1).reshape(X.shape[0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction utilitaire pour calculer relu($x$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x, np.zeros(x.shape))\n",
    "\n",
    "def onehot(m, y):\n",
    "    return np.eye(m)[y]\n",
    "\n",
    "def onehot_matrix(m, targets):\n",
    "    \"\"\"\n",
    "    Returns: onehot matrix where every column is a onehot vector of the coressponding target\n",
    "    \"\"\"\n",
    "    eye = np.eye(m)\n",
    "    onehot_matrix = np.zeros((m,len(targets)))\n",
    "    \n",
    "    for i, y in enumerate(targets):\n",
    "        onehot_matrix[:,i] = eye[y]\n",
    "        \n",
    "    return onehot_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Calcul du gradient sur un exemple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation de fprop et bprop pour calculer le gradient sur un exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    \n",
    "    def __init__(self, n_input, n_hidden, n_out):\n",
    "        \n",
    "        self.n_in = n_input\n",
    "        self.n_h = n_hidden\n",
    "        self.n_o = n_out\n",
    "        \n",
    "        low_bound = -1 / np.sqrt([self.n_in, self.n_h])\n",
    "        up_bound = 1 / np.sqrt([self.n_in, self.n_h])\n",
    "        \n",
    "        # Initialize the parameters\n",
    "        self.W1 = np.random.uniform(low_bound[0], up_bound[0], size=(self.n_h, self.n_in))  # d_h x d\n",
    "        self.W2 = np.random.uniform(low_bound[1], up_bound[1], size=(self.n_o, self.n_h))  # m x d_h\n",
    "        self.b1 = np.zeros(self.n_h)  # dimension d_h\n",
    "        self.b2 = np.zeros(self.n_o) # dimension m\n",
    "    \n",
    "    def fprop(self, x):\n",
    "        '''Computes activations for every layer'''\n",
    "        self.ha = self.W1.dot(x) + self.b1\n",
    "        self.hs = relu(self.ha)\n",
    "        self.oa = self.W2.dot(self.hs) + self.b2\n",
    "        self.os = softmax_vector(self.oa)\n",
    "            \n",
    "    def bprop(self, x, y):\n",
    "        '''Computes the gradients, must be executed after fprop'''\n",
    "                      \n",
    "        grad_oa = self.os - onehot(self.n_o, y)\n",
    "        grad_b2 = grad_oa\n",
    "        grad_W2 = np.outer(grad_oa, self.hs)\n",
    "        grad_hs = self.W2.T.dot(grad_oa)\n",
    "        grad_ha = grad_hs * (self.ha > 0)\n",
    "        grad_W1 = np.outer(grad_ha, x)\n",
    "        grad_b1 = grad_ha\n",
    "        \n",
    "        return grad_W1, grad_W2, grad_b1, grad_b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vérification du gradient par différences finies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant vérifier que le gradient calculé par Keras est juste. Pour cela on va calculer une approximation du gradient en utilisant la méthode de la différence finie.\n",
    "\n",
    "Pour chaque composante de w1, on va:\n",
    " 1. calculer la valeur de la fonction objectif $L$\n",
    " 2. ajouter une petite valeur $\\epsilon$ à la composante\n",
    " 3. recalculer la valeur de la fonction objectif $L'$\n",
    " 4. remettre à l'ancienne valeur la composante (c'est à dire soustraire $\\epsilon$)\n",
    "\n",
    "Le gradient par différences finies sera dans ce cas donné par:\n",
    "$\\Big( \\frac{\\partial L}{\\partial W_{1}} \\Big)_{ij} = \\frac{1}{\\epsilon} (L' - L)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(x, y, W1, W2, b1, b2):\n",
    "    ha = W1.dot(x) + b1\n",
    "    hs = relu(ha)\n",
    "    oa = W2.dot(hs) + b2\n",
    "    os = softmax_vector(oa)\n",
    "    \n",
    "    return -np.log(os[y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction pour calculer le gradient par differences finies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def finite_diff(x, y, neural_net, eps=1e-5):\n",
    "    \n",
    "    # params\n",
    "    W1 = neural_net.W1\n",
    "    W2 = neural_net.W2\n",
    "    b1 = neural_net.b1\n",
    "    b2 = neural_net.b2\n",
    "    \n",
    "    # gradients\n",
    "    grad_w1_diff = np.zeros(W1.shape)\n",
    "    grad_w2_diff = np.zeros(W2.shape)\n",
    "    grad_b1_diff = np.zeros(b1.shape)\n",
    "    grad_b2_diff = np.zeros(b2.shape)\n",
    "    \n",
    "    neural_net.fprop(x)\n",
    "    loss = compute_loss(x, y, W1, W2, b1, b2)\n",
    "    \n",
    "    for i in range(W1.shape[0]):\n",
    "        for j in range(W1.shape[1]):\n",
    "            W1[i,j] = W1[i,j] + eps\n",
    "            loss_prime = compute_loss(x, y, W1, W2, b1, b2)\n",
    "            grad_w1_diff[i, j] = (loss_prime - loss) / epsilon\n",
    "            W1[i,j] = W1[i,j] - eps\n",
    "    for i in range(W2.shape[0]):\n",
    "        for j in range(W2.shape[1]):\n",
    "            W2[i,j] = W2[i,j] + eps\n",
    "            loss_prime = compute_loss(x, y, W1, W2, b1, b2)\n",
    "            grad_w2_diff[i, j] = (loss_prime - loss) / epsilon\n",
    "            W2[i,j] = W2[i,j] - eps\n",
    "    for i in range(b1.shape[0]):\n",
    "            b1[i] = b1[i] + eps\n",
    "            loss_prime = compute_loss(x, y, W1, W2, b1, b2)\n",
    "            grad_b1_diff[i] = (loss_prime - loss) / epsilon\n",
    "            b1[i] = b1[i] - eps\n",
    "    for i in range(b1.shape[0]):\n",
    "            b2[i] = b2[i] + eps\n",
    "            loss_prime = compute_loss(x, y, W1, W2, b1, b2)\n",
    "            grad_b2_diff[i] = (loss_prime - loss) / epsilon\n",
    "            b2[i] = b2[i] - eps\n",
    "            \n",
    "    return grad_w1_diff, grad_w2_diff, grad_b1_diff, grad_b2_diff\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage de vérification du gradient par différence finie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt(open('2moons.txt','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.08253842  0.40562004]\n",
      " [ 0.          0.        ]]\n",
      "[[-0.08253849  0.40561855]\n",
      " [ 0.         -0.        ]]\n",
      "[[ -6.18492098e-08  -1.49364136e-06]\n",
      " [  0.00000000e+00  -0.00000000e+00]]\n",
      "[[ -1.00711679e-06  -0.00000000e+00]\n",
      " [ -1.00712198e-06   0.00000000e+00]]\n",
      "[ -4.63207366e-07   0.00000000e+00]\n",
      "[ -1.14510311e-06  -1.14509825e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zimmae\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "C:\\Users\\zimmae\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNet(2, 2, 2)\n",
    "x = data[0,:-1]\n",
    "y = data[0,-1]\n",
    "epsilon = 1e-5\n",
    "\n",
    "# gradients par différence finie\n",
    "grad_w1_diff, grad_w2_diff, grad_b1_diff, grad_b2_diff = finite_diff(x, y, nn, epsilon)\n",
    "\n",
    "# gradients par implémentation dans classe neural_net\n",
    "grad_W1, grad_W2, grad_b1, grad_b2 = nn.bprop(x,y)\n",
    "\n",
    "# affichage de la différence\n",
    "print(grad_w1_diff)\n",
    "print(grad_W1)\n",
    "print(grad_W1 - grad_w1_diff)\n",
    "print(grad_W2 - grad_w2_diff)\n",
    "print(grad_b1 - grad_b1_diff)\n",
    "print(grad_b2 - grad_b2_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ajout de hyperparamètre de taille de lot K\n",
    "La fonction bprop est modifiée pour calculer le gradient sur un batch d'exemple. La fonction train est également ajoutée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    \n",
    "    def __init__(self, n_input, n_hidden, n_out, lambdas, K = 1):\n",
    "        \n",
    "        self.n_in = n_input\n",
    "        self.n_h = n_hidden\n",
    "        self.n_o = n_out\n",
    "        self.K = K\n",
    "        self.lambdas = lambdas\n",
    "        \n",
    "        low_bound = -1 / np.sqrt([self.n_in, self.n_h])\n",
    "        up_bound = 1 / np.sqrt([self.n_in, self.n_h])\n",
    "        \n",
    "        # Initialize the parameters\n",
    "        self.W1 = np.random.uniform(low_bound[0], up_bound[0], size=(self.n_h, self.n_in))  # d_h x d\n",
    "        self.W2 = np.random.uniform(low_bound[1], up_bound[1], size=(self.n_o, self.n_h))  # m x d_h\n",
    "        self.b1 = np.zeros(self.n_h)  # dimension d_h\n",
    "        self.b2 = np.zeros(self.n_o) # dimension m\n",
    "    \n",
    "    def fprop(self, x):\n",
    "        '''Computes activations for every layer'''\n",
    "        self.ha = self.W1.dot(x) + self.b1\n",
    "        self.hs = relu(self.ha)\n",
    "        self.oa = self.W2.dot(self.hs) + self.b2\n",
    "        self.os = softmax_vector(self.oa)\n",
    "            \n",
    "    def bprop(self, X, Y):\n",
    "        '''Computes the gradients over all examples in (X,Y) with loop'''\n",
    "        grad_W1_mean = np.zeros((self.n_h, self.n_in))\n",
    "        grad_W2_mean = np.zeros((self.n_o, self.n_h))\n",
    "        grad_b1_mean = np.zeros(self.n_h)\n",
    "        grad_b2_mean = np.zeros(self.n_o)\n",
    "        \n",
    "        n = X.shape[0]\n",
    "        \n",
    "        for i in range(n):\n",
    "            \n",
    "            self.fprop(X[i,:])\n",
    "            \n",
    "            grad_oa = self.os - onehot(self.n_o, Y[i])\n",
    "            grad_b2 = grad_oa\n",
    "            grad_W2 = np.outer(grad_oa, self.hs)\n",
    "            grad_hs = self.W2.T.dot(grad_oa)\n",
    "            grad_ha = grad_hs * (self.ha > 0)\n",
    "            grad_W1 = np.outer(grad_ha, X[i,:])\n",
    "            grad_b1 = grad_ha\n",
    "            \n",
    "            grad_W1_mean = grad_W1_mean + grad_W1 / n\n",
    "            grad_W2_mean = grad_W2_mean + grad_W2 / n\n",
    "            grad_b1_mean = grad_b1_mean + grad_b1 / n\n",
    "            grad_b2_mean = grad_b2_mean + grad_b2 / n\n",
    "        \n",
    "        return grad_W1_mean, grad_W2_mean, grad_b1_mean, grad_b2_mean\n",
    "    \n",
    "    def compute_loss(self, y):\n",
    "        return -np.log(self.os[y])\n",
    "    \n",
    "    def train(self, train_data, max_iter, eta=0.05):\n",
    "        \n",
    "        n_batches = int(np.ceil(train_data.shape[0]/self.K)) # number of batches\n",
    "        \n",
    "        # Initialize batch start and end indices\n",
    "        batch_start = 0\n",
    "        if (batch_start + self.K < train_data.shape[0]):\n",
    "            batch_end = batch_start + self.K\n",
    "        else:\n",
    "            batch_end = train_data.shape[0]\n",
    "        \n",
    "        for i in range(max_iter):\n",
    "            for j in range(n_batches):\n",
    "                batch = train_data[batch_start:batch_end]\n",
    "                \n",
    "                grad_W1_mean, grad_W2_mean, grad_b1_mean, grad_b2_mean = self.bprop(batch[:,:-1], batch[:,-1]) \n",
    "                \n",
    "                n = len(batch)\n",
    "                \n",
    "                #regularization\n",
    "                penality_grad_W1 = self.lambdas[0][0] * np.sign(self.W1) + 2 * self.lambdas[0][1] * self.W1\n",
    "                penality_grad_W2 = self.lambdas[1][0] * np.sign(self.W2) + 2 * self.lambdas[1][1] * self.W2\n",
    "                \n",
    "                self.W1 = self.W1 - eta * (grad_W1_mean + penality_grad_W1)\n",
    "                self.W2 = self.W2 - eta * (grad_W2_mean + penality_grad_W2)\n",
    "                self.b1 = self.b1 - eta * grad_b1_mean\n",
    "                self.b2 = self.b2 - eta * grad_b2_mean\n",
    "                \n",
    "                # Get next batch\n",
    "                batch_start = batch_end + 1        \n",
    "                if (batch_start + self.K < train_data.shape[0]):\n",
    "                    batch_end = batch_start + self.K\n",
    "                else:\n",
    "                    batch_end = train_data.shape[0]\n",
    "                    \n",
    "    def compute_predictions(self, test_data):\n",
    "\n",
    "        pred = np.empty((test_data.shape[0],self.n_o))\n",
    "\n",
    "        for i in range(test_data.shape[0]):\n",
    "            self.fprop(test_data[i,:])\n",
    "            pred[i,:] = self.os\n",
    "\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vérification du gradient par différence finie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage de vérification du gradient pour un lot de 10 exemples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00108852 -0.07567956]\n",
      " [ 0.03104294 -0.08076362]]\n",
      "[[-0.00108853 -0.07567969]\n",
      " [ 0.03104289 -0.08076379]]\n",
      "[[ -8.24450934e-09  -1.30139726e-07]\n",
      " [ -4.35909916e-08  -1.66439810e-07]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zimmae\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "C:\\Users\\zimmae\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "lambdas = np.array([[0, 0],\n",
    "       [0, 0]])\n",
    "nn = NeuralNet(2, 2, 2, lambdas)\n",
    "X = data[0:10,:-1]\n",
    "Y = data[0:10,-1]\n",
    "\n",
    "# params\n",
    "W1 = nn.W1\n",
    "W2 = nn.W2\n",
    "b1 = nn.b1\n",
    "b2 = nn.b2\n",
    "\n",
    "# gradients par différence finie\n",
    "grad_w1_diff = np.zeros(W1.shape)\n",
    "grad_w2_diff = np.zeros(W2.shape)\n",
    "grad_b1_diff = np.zeros(b1.shape)\n",
    "grad_b2_diff = np.zeros(b2.shape)\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    grad_w1, grad_w2, grad_b1, grad_b2 = finite_diff(X[i,:], Y[i], nn, epsilon)\n",
    "    grad_w1_diff += grad_w1 / X.shape[0]\n",
    "    grad_w2_diff += grad_w2 / X.shape[0]\n",
    "    grad_b1_diff += grad_b1 / X.shape[0]\n",
    "    grad_b2_diff += grad_b2 / X.shape[0]\n",
    "    \n",
    "# gradients par implémentation dans classe neural_net    \n",
    "grad_W1, grad_W2, grad_b1, grad_b2 = nn.bprop(X,Y)\n",
    "\n",
    "# affichage de la différence\n",
    "print(grad_w1_diff)\n",
    "print(grad_W1)\n",
    "print(grad_W1 - grad_w1_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entraînement sur 2 moons et visualisation des régions de décision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction de calcul du taux d'erreur de classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cette fonction renvoie le taux d'erreur étant donné un classifieur et un ensemble de données\n",
    "def taux_erreur(classifieur, data):\n",
    "    data_prob = classifieur.compute_predictions(data[:,:-1])\n",
    "    data_classe_pred = np.argmax(data_prob, axis = 1)\n",
    "    #print(data_prob[1:20,:])\n",
    "    #print(data_classe_pred[1:20])\n",
    "    return 100*float(sum(sum([data_classe_pred != data[:,-1]])))/data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entraînement sur les données 2 moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zimmae\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.51630435  0.48369565  0.51630435  0.48369565  0.48369565  0.48369565\n",
      "   0.48369565  0.48369565  0.51630435  0.51630435]\n",
      " [ 0.28532609  0.2201087   0.48369565  0.51630435  0.48369565  0.48369565\n",
      "   0.48369565  0.48369565  0.51630435  0.48369565]\n",
      " [ 0.23097826  0.27717391  0.51358696  0.36413043  0.48369565  0.48369565\n",
      "   0.48369565  0.51630435  0.51630435  0.48369565]\n",
      " [ 0.30163043  0.24184783  0.24184783  0.48369565  0.2798913   0.48369565\n",
      "   0.51630435  0.51630435  0.51630435  0.51630435]\n",
      " [ 0.2826087   0.2826087   0.26086957  0.32065217  0.25543478  0.42391304\n",
      "   0.51630435  0.51630435  0.48369565  0.48369565]\n",
      " [ 0.3423913   0.39130435  0.24456522  0.36141304  0.30706522  0.48369565\n",
      "   0.23369565  0.48369565  0.51630435  0.48369565]\n",
      " [ 0.25543478  0.2201087   0.32608696  0.25        0.26358696  0.51630435\n",
      "   0.48369565  0.51630435  0.48369565  0.48369565]\n",
      " [ 0.4701087   0.22282609  0.25271739  0.23913043  0.30163043  0.29891304\n",
      "   0.3125      0.30978261  0.51630435  0.51630435]\n",
      " [ 0.42663043  0.2173913   0.25271739  0.31521739  0.29619565  0.26358696\n",
      "   0.25543478  0.48369565  0.51630435  0.2826087 ]\n",
      " [ 0.2798913   0.33695652  0.31521739  0.27445652  0.48641304  0.51086957\n",
      "   0.50543478  0.51630435  0.25271739  0.3423913 ]\n",
      " [ 0.25543478  0.30978261  0.29891304  0.29891304  0.29891304  0.3423913\n",
      "   0.40217391  0.51630435  0.51630435  0.23369565]\n",
      " [ 0.51630435  0.30163043  0.25        0.26902174  0.22554348  0.33423913\n",
      "   0.20923913  0.22282609  0.26086957  0.33423913]\n",
      " [ 0.2173913   0.20108696  0.31521739  0.21467391  0.25271739  0.23913043\n",
      "   0.35054348  0.29347826  0.51630435  0.48369565]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "ntrain = 2* (data.shape[0] // 3)\n",
    "\n",
    "inds = [i for i in range(data.shape[0])]\n",
    "np.random.seed(123)\n",
    "np.random.shuffle(inds)\n",
    "\n",
    "test_inds = inds[:ntrain]\n",
    "validation_inds = inds[ntrain:]\n",
    "\n",
    "#On définit l'ensemble d'entraînement et l'ensemble de validation\n",
    "train_data = data[test_inds,]\n",
    "validation_data = data[validation_inds,]\n",
    "\n",
    "#Variation de la valeur des hyperparamètres K et eta\n",
    "lambdas = np.array([[0.001, 0.001],\n",
    "       [0.001, 0.001]])\n",
    "\n",
    "def frange(start, stop, step):\n",
    "    i = start\n",
    "    while i < stop:\n",
    "        yield i\n",
    "        i += step\n",
    "        \n",
    "erreurs = np.zeros([15,10])\n",
    "i = 0\n",
    "for K in range(50,700,50):\n",
    "    j = 0\n",
    "    for eta in frange(0.1,1,0.1):\n",
    "        nn = NeuralNet(2,20,2,lambdas, K)   \n",
    "        nn.train(train_data,100,eta)  \n",
    "        #print(nn.W1)\n",
    "        #print(nn.W2)\n",
    "        #print(nn.b1)\n",
    "        #print(nn.b2)\n",
    "        erreurs[i,j] = 0.01*taux_erreur(nn, validation_data)\n",
    "        j = j + 1\n",
    "    i = i + 1\n",
    "print(erreurs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optimisation du calcul de gradient pour mini-batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{W}^{(1)} \\in \\mathbb{R}^{d_h \\times d}$, $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ et $\\mathbf{B}^{(1)} \\in \\mathbb{R}^{d_h \\times n}$\n",
    "\n",
    "$$\\mathbf{h}^{a} = \\mathbf{W}^{(1)}\\mathbf{X}^{\\top} + \\mathbf{B}^{(1)} \\in \\mathbb{R}^{d_h \\times n}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetVectorized:\n",
    "    \n",
    "    def __init__(self, n_input, n_hidden, n_out, lambdas, K=1):\n",
    "        \n",
    "        self.n_in = n_input\n",
    "        self.n_h = n_hidden\n",
    "        self.n_o = n_out\n",
    "        self.lambdas = lambdas\n",
    "        self.K = K\n",
    "        \n",
    "        low_bound = -1 / np.sqrt([self.n_in, self.n_h])\n",
    "        up_bound = 1 / np.sqrt([self.n_in, self.n_h])\n",
    "        \n",
    "        # Initialize the parameters\n",
    "        self.W1 = np.random.uniform(low_bound[0], up_bound[0], size=(self.n_h, self.n_in))  # d_h x d\n",
    "        self.W2 = np.random.uniform(low_bound[1], up_bound[1], size=(self.n_o, self.n_h))  # m x d_h\n",
    "        self.b1 = np.zeros(self.n_h)  # dimension d_h\n",
    "        self.b2 = np.zeros(self.n_o) # dimension m\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        '''\n",
    "        Computes activations for every layer\n",
    "        X: input data set\n",
    "        '''\n",
    "        self.ha = self.W1.dot(X.T) + self.b1.reshape(self.n_h, 1)\n",
    "        self.hs = relu(ha)\n",
    "        self.oa = self.W2.dot(hs) + self.b2.reshape(self.n_o, 1)\n",
    "        self.os = softmax(oa)\n",
    "            \n",
    "    def bprop(self, X, Y):\n",
    "        '''\n",
    "        Computes the gradients, must be executed after fprop\n",
    "        X: Input data set\n",
    "        Y: targets\n",
    "        '''\n",
    "                      \n",
    "        grad_oa = os - onehot_matrix(self.n_out, Y)\n",
    "        grad_b2 = grad_oa # m x n\n",
    "        grad_W2 = np.dot(grad_oa, self.hs.T) # sum of gradients grad_W2 for each example\n",
    "        grad_hs = self.W2.T.dot(grad_oa) # d_h x n\n",
    "        grad_ha = grad_hs * (self.ha > 0) # d_h x n\n",
    "        grad_W1 = np.dot(grad_ha, X.T) # sum of gradients grad_W1 for each example\n",
    "        grad_b1 = grad_ha # d_h x n\n",
    "        \n",
    "        return grad_W1, grad_W2, grad_b1, grad_b2\n",
    "    \n",
    "    def compute_loss(self, y):\n",
    "        # TODO: adjust for y vector\n",
    "        return -np.log(self.os)\n",
    "    \n",
    "    def train(self, train_data, max_iter, eta=0.05):\n",
    "        \n",
    "        n_batches = int(np.ceil(train_data.shape[0]/self.K)) # number of batches\n",
    "        \n",
    "        # Initialize batch start and end indices\n",
    "        batch_start = 0\n",
    "        if (batch_start + self.K < train_data.shape[0]):\n",
    "            batch_end = batch_start + self.K\n",
    "        else:\n",
    "            batch_end = train_data.shape[0]\n",
    "        \n",
    "        for i in range(max_iter):\n",
    "            for j in range(n_batches):\n",
    "                \n",
    "                batch = train_data[batch_start:batch_end]\n",
    "                \n",
    "                self.fprop(batch[:,:-1])\n",
    "                grad_W1, grad_W2, grad_b1, grad_b2 = self.bprop(batch[:,:-1], batch[:,-1]) \n",
    "                \n",
    "                n = len(batch)\n",
    "                \n",
    "                #regularization\n",
    "                penality_grad_W1 = self.lambdas[0][0] * np.sign(self.W1) + 2 * self.lambdas[0][1] * self.W1\n",
    "                penality_grad_W2 = self.lambdas[1][0] * np.sign(self.W2) + 2 * self.lambdas[1][1] * self.W2\n",
    "                \n",
    "                self.W1 = self.W1 - eta * ((grad_W1 / n) + penality_grad_W1)\n",
    "                self.W2 = self.W2 - eta * ((grad_W2 / n) + penality_grad_W2)\n",
    "                self.b1 = self.b1 - eta * np.mean(grad_b1, axis=1)\n",
    "                self.b2 = self.b2 - eta * np.mean(grad_b2, axis=1)\n",
    "                \n",
    "                \n",
    "                # Get next batch\n",
    "                batch_start = batch_end + 1        \n",
    "                if (batch_start + batch_size < train_data.shape[0]):\n",
    "                    batch_end = batch_start + batch_size\n",
    "                else:\n",
    "                    batch_end = train_data.shape[0]\n",
    "                    \n",
    "    def compute_predictions(self, test_data):\n",
    "\n",
    "        pred = np.empty((test_data.shape[0],self.n_o))\n",
    "\n",
    "        for i in range(test_data.shape[0]):\n",
    "            self.fprop(test_data[i,:])\n",
    "            pred[i,:] = self.os\n",
    "\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 7. Comparaison des deux implémentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparaison du temps de calcul pour une époque quand K=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
