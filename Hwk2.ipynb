{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implémentation du réseau de neurones et expérimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Implémentation de fonctions utiles\n",
    "Calcul numériquement stable du softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_vector(x):\n",
    "    '''\n",
    "    x:  is a data vector.\n",
    "    returns: the result of the softmax function applied to the data vector.\n",
    "    '''\n",
    "    max_comp = np.amax(x)\n",
    "    normalized  = x - max_comp\n",
    "    \n",
    "    exponential = np.exp(normalized)\n",
    "    \n",
    "    return exponential/np.sum(exponential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    '''\n",
    "    X: matrix that holds the data, every row is a data vector.\n",
    "    returns: matrix where every row is the result of the softmax function applied to the corresponding data vector.\n",
    "    '''\n",
    "    \n",
    "    max_comp = np.amax(X, axis=1)\n",
    "    normalized  = X - max_comp.reshape(X.shape[0], 1)\n",
    "    \n",
    "    exponential = np.exp(normalized)\n",
    "    \n",
    "    return exponential/np.sum(exponential, axis=1).reshape(X.shape[0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction utilitaire pour calculer relu($x$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x, np.zeros(x.shape))\n",
    "\n",
    "def onehot(m, y):\n",
    "    return np.eye(m)[y]\n",
    "\n",
    "def onehot_matrix(m, targets):\n",
    "    \"\"\"\n",
    "    Returns: onehot matrix where every column is a onehot vector of the coressponding target\n",
    "    \"\"\"\n",
    "    eye = np.eye(m)\n",
    "    onehot_matrix = np.zeros((m,len(targets)))\n",
    "    \n",
    "    for i, y in enumerate(targets):\n",
    "        onehot_matrix[:,i] = eye[y]\n",
    "        \n",
    "    return onehot_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Calcul du gradient sur un exemple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation de fprop et bprop pour calculer le gradient sur un exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNet_basic:\n",
    "    \n",
    "    def __init__(self, n_input, n_hidden, n_out):\n",
    "        \n",
    "        self.n_in = n_input\n",
    "        self.n_h = n_hidden\n",
    "        self.n_o = n_out\n",
    "        \n",
    "        low_bound = -1 / np.sqrt([self.n_in, self.n_h])\n",
    "        up_bound = 1 / np.sqrt([self.n_in, self.n_h])\n",
    "        \n",
    "        # Initialize the parameters\n",
    "        self.W1 = np.random.uniform(low_bound[0], up_bound[0], size=(self.n_h, self.n_in))  # d_h x d\n",
    "        self.W2 = np.random.uniform(low_bound[1], up_bound[1], size=(self.n_o, self.n_h))  # m x d_h\n",
    "        self.b1 = np.zeros(self.n_h)  # dimension d_h\n",
    "        self.b2 = np.zeros(self.n_o) # dimension m\n",
    "    \n",
    "    def fprop(self, x):\n",
    "        '''Computes activations for every layer'''\n",
    "        self.ha = self.W1.dot(x) + self.b1\n",
    "        self.hs = relu(self.ha)\n",
    "        self.oa = self.W2.dot(self.hs) + self.b2\n",
    "        self.os = softmax_vector(self.oa)\n",
    "            \n",
    "    def bprop(self, x, y):\n",
    "        '''Computes the gradients, must be executed after fprop'''\n",
    "                      \n",
    "        grad_oa = self.os - onehot(self.n_o, y)\n",
    "        grad_b2 = grad_oa\n",
    "        grad_W2 = np.outer(grad_oa, self.hs)\n",
    "        grad_hs = self.W2.T.dot(grad_oa)\n",
    "        grad_ha = grad_hs * (self.ha > 0)\n",
    "        grad_W1 = np.outer(grad_ha, x)\n",
    "        grad_b1 = grad_ha\n",
    "        \n",
    "        return grad_W1, grad_W2, grad_b1, grad_b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vérification du gradient par différences finies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant vérifier que le gradient calculé est juste. Pour cela on va calculer une approximation du gradient en utilisant la méthode de la différence finie.\n",
    "\n",
    "Pour chaque composante de w1, on va:\n",
    " 1. calculer la valeur de la fonction objectif $L$\n",
    " 2. ajouter une petite valeur $\\epsilon$ à la composante\n",
    " 3. recalculer la valeur de la fonction objectif $L'$\n",
    " 4. remettre à l'ancienne valeur la composante (c'est à dire soustraire $\\epsilon$)\n",
    "\n",
    "Le gradient par différences finies sera dans ce cas donné par:\n",
    "$\\Big( \\frac{\\partial L}{\\partial W_{1}} \\Big)_{ij} = \\frac{1}{\\epsilon} (L' - L)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(x, y, W1, W2, b1, b2):\n",
    "    ha = W1.dot(x) + b1\n",
    "    hs = relu(ha)\n",
    "    oa = W2.dot(hs) + b2\n",
    "    os = softmax_vector(oa)\n",
    "    \n",
    "    return -np.log(os[y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction pour calculer le gradient par differences finies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def finite_diff(x, y, neural_net, eps=1e-5):\n",
    "    \n",
    "    # params\n",
    "    W1 = neural_net.W1\n",
    "    W2 = neural_net.W2\n",
    "    b1 = neural_net.b1\n",
    "    b2 = neural_net.b2\n",
    "    \n",
    "    # gradients\n",
    "    grad_w1_diff = np.zeros(W1.shape)\n",
    "    grad_w2_diff = np.zeros(W2.shape)\n",
    "    grad_b1_diff = np.zeros(b1.shape)\n",
    "    grad_b2_diff = np.zeros(b2.shape)\n",
    "    \n",
    "    neural_net.fprop(x)\n",
    "    loss = compute_loss(x, y, W1, W2, b1, b2)\n",
    "    \n",
    "    for i in range(W1.shape[0]):\n",
    "        for j in range(W1.shape[1]):\n",
    "            W1[i,j] = W1[i,j] + eps\n",
    "            loss_prime = compute_loss(x, y, W1, W2, b1, b2)\n",
    "            grad_w1_diff[i, j] = (loss_prime - loss) / epsilon\n",
    "            W1[i,j] = W1[i,j] - eps\n",
    "    for i in range(W2.shape[0]):\n",
    "        for j in range(W2.shape[1]):\n",
    "            W2[i,j] = W2[i,j] + eps\n",
    "            loss_prime = compute_loss(x, y, W1, W2, b1, b2)\n",
    "            grad_w2_diff[i, j] = (loss_prime - loss) / epsilon\n",
    "            W2[i,j] = W2[i,j] - eps\n",
    "    for i in range(b1.shape[0]):\n",
    "            b1[i] = b1[i] + eps\n",
    "            loss_prime = compute_loss(x, y, W1, W2, b1, b2)\n",
    "            grad_b1_diff[i] = (loss_prime - loss) / epsilon\n",
    "            b1[i] = b1[i] - eps\n",
    "    for i in range(b1.shape[0]):\n",
    "            b2[i] = b2[i] + eps\n",
    "            loss_prime = compute_loss(x, y, W1, W2, b1, b2)\n",
    "            grad_b2_diff[i] = (loss_prime - loss) / epsilon\n",
    "            b2[i] = b2[i] - eps\n",
    "            \n",
    "    return grad_w1_diff, grad_w2_diff, grad_b1_diff, grad_b2_diff\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage de vérification du gradient par différence finie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt(open('2moons.txt','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Différence entre les deux gradients pour W1, W2, b1, b2:\n",
      "[[ -5.58324467e-10  -1.32355634e-08]\n",
      " [  0.00000000e+00  -0.00000000e+00]]\n",
      "[[ -8.72016346e-08  -0.00000000e+00]\n",
      " [ -8.72033004e-08   0.00000000e+00]]\n",
      "[ -4.11289126e-09   0.00000000e+00]\n",
      "[ -1.24994118e-06  -1.24992580e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zimmae\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "C:\\Users\\zimmae\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNet_basic(2, 2, 2)\n",
    "x = data[0,:-1]\n",
    "y = data[0,-1]\n",
    "epsilon = 1e-5\n",
    "\n",
    "# gradients par différence finie\n",
    "grad_w1_diff, grad_w2_diff, grad_b1_diff, grad_b2_diff = finite_diff(x, y, nn, epsilon)\n",
    "\n",
    "# gradients par implémentation dans classe neural_net\n",
    "grad_W1, grad_W2, grad_b1, grad_b2 = nn.bprop(x,y)\n",
    "\n",
    "# affichage de la différence\n",
    "print('Différence entre les deux gradients pour W1, W2, b1, b2:')\n",
    "#print(grad_w1_diff)\n",
    "#print(grad_W1)\n",
    "print(grad_W1 - grad_w1_diff)\n",
    "print(grad_W2 - grad_w2_diff)\n",
    "print(grad_b1 - grad_b1_diff)\n",
    "print(grad_b2 - grad_b2_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ajout de hyperparamètre de taille de lot K\n",
    "La fonction bprop est modifiée pour calculer le gradient sur un batch d'exemple. La fonction train est également ajoutée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NeuralNet_loop:\n",
    "    \n",
    "    def __init__(self, n_input, n_hidden, n_out, lambdas, K = 1):\n",
    "        \n",
    "        self.n_in = n_input\n",
    "        self.n_h = n_hidden\n",
    "        self.n_o = n_out\n",
    "        self.K = K\n",
    "        self.lambdas = lambdas\n",
    "        \n",
    "        low_bound = -1 / np.sqrt([self.n_in, self.n_h])\n",
    "        up_bound = 1 / np.sqrt([self.n_in, self.n_h])\n",
    "        \n",
    "        np.random.seed(123)\n",
    "        \n",
    "        # Initialize the parameters\n",
    "        self.W1 = np.random.uniform(low_bound[0], up_bound[0], size=(self.n_h, self.n_in))  # d_h x d\n",
    "        self.W2 = np.random.uniform(low_bound[1], up_bound[1], size=(self.n_o, self.n_h))  # m x d_h\n",
    "        self.b1 = np.zeros(self.n_h)  # dimension d_h\n",
    "        self.b2 = np.zeros(self.n_o) # dimension m\n",
    "    \n",
    "    def fprop(self, x):\n",
    "        '''Computes activations for every layer'''\n",
    "        ha = self.W1.dot(x) + self.b1\n",
    "        hs = relu(ha)\n",
    "        oa = self.W2.dot(hs) + self.b2\n",
    "        os = softmax_vector(oa)\n",
    "        return ha, hs, oa, os\n",
    "            \n",
    "    def bprop(self, X, Y):\n",
    "        '''Computes the gradients over all examples in (X,Y) with loop'''\n",
    "        grad_W1_mean = np.zeros((self.n_h, self.n_in))\n",
    "        grad_W2_mean = np.zeros((self.n_o, self.n_h))\n",
    "        grad_b1_mean = np.zeros(self.n_h)\n",
    "        grad_b2_mean = np.zeros(self.n_o)\n",
    "        \n",
    "        n = X.shape[0]\n",
    "        loss = 0.\n",
    "        \n",
    "        for i in range(n):\n",
    "            \n",
    "            ha, hs, oa, os = self.fprop(X[i,:])\n",
    "            loss += -np.log(os[Y[i]])\n",
    "            \n",
    "            grad_oa = os - onehot(self.n_o, Y[i])\n",
    "            grad_b2 = grad_oa\n",
    "            grad_W2 = np.outer(grad_oa, hs)\n",
    "            grad_hs = self.W2.T.dot(grad_oa)\n",
    "            grad_ha = grad_hs * (ha > 0)\n",
    "            grad_W1 = np.outer(grad_ha, X[i,:])\n",
    "            grad_b1 = grad_ha\n",
    "            \n",
    "            grad_W1_mean = grad_W1_mean + grad_W1 / n\n",
    "            grad_W2_mean = grad_W2_mean + grad_W2 / n\n",
    "            grad_b1_mean = grad_b1_mean + grad_b1 / n\n",
    "            grad_b2_mean = grad_b2_mean + grad_b2 / n\n",
    "        \n",
    "        return grad_W1_mean, grad_W2_mean, grad_b1_mean, grad_b2_mean, loss \n",
    "    \n",
    "    def compute_loss(self, x, y):\n",
    "        _, _, _, os = self.fprop(x)\n",
    "        return -np.log(self.os[y])\n",
    "    \n",
    "    def train(self, train_data, max_iter, eta=0.05):\n",
    "        \n",
    "        n_batches = int(np.ceil(train_data.shape[0]/self.K)) # number of batches\n",
    "        \n",
    "        # Initialize batch start and end indices\n",
    "        \n",
    "        for i in range(max_iter):\n",
    "            loss = 0.\n",
    "            for j in range(0,train_data.shape[0],self.K):\n",
    "                batch = train_data[j:j+self.K]\n",
    "                \n",
    "                grad_W1_mean, grad_W2_mean, grad_b1_mean, grad_b2_mean, bloss = self.bprop(batch[:,:-1], batch[:,-1]) \n",
    "                loss += bloss\n",
    "                \n",
    "                n = len(batch)\n",
    "                \n",
    "                #regularization\n",
    "                penality_grad_W1 = self.lambdas[0][0] * np.sign(self.W1) + 2 * self.lambdas[0][1] * self.W1\n",
    "                penality_grad_W2 = self.lambdas[1][0] * np.sign(self.W2) + 2 * self.lambdas[1][1] * self.W2\n",
    "                \n",
    "                self.W1 = self.W1 - eta * (grad_W1_mean + penality_grad_W1)\n",
    "                self.W2 = self.W2 - eta * (grad_W2_mean + penality_grad_W2)\n",
    "                self.b1 = self.b1 - eta * grad_b1_mean\n",
    "                self.b2 = self.b2 - eta * grad_b2_mean\n",
    "                \n",
    "            #if i%100 == 0:\n",
    "                #print('The loss after epoque ', i, ' is ', loss / train_data.shape[0])\n",
    "                    \n",
    "    def compute_predictions(self, test_data):\n",
    "\n",
    "        pred = np.empty((test_data.shape[0],self.n_o))\n",
    "\n",
    "        for i in range(test_data.shape[0]):\n",
    "            _, _, _, os = self.fprop(test_data[i,:])\n",
    "            pred[i,:] = os\n",
    "\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vérification du gradient par différence finie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage de vérification du gradient pour un lot de 10 exemples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Différence entre les deux gradients pour W1, W2, b1, b2\n",
      "[[ -1.37831296e-08  -1.77901403e-07]\n",
      " [ -4.38070745e-08  -1.18092063e-07]]\n",
      "[[ -1.41384634e-07  -4.08140944e-08]\n",
      " [ -1.41384729e-07  -4.08172739e-08]]\n",
      "[ -6.75580648e-08  -6.82009196e-08]\n",
      "[ -1.24341350e-06  -1.24341613e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zimmae\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "C:\\Users\\zimmae\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:43: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "C:\\Users\\zimmae\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "lambdas = np.array([[0, 0],\n",
    "       [0, 0]])\n",
    "nn = NeuralNet_loop(2, 2, 2, lambdas)\n",
    "X = data[0:10,:-1]\n",
    "Y = data[0:10,-1]\n",
    "\n",
    "# params\n",
    "W1 = nn.W1\n",
    "W2 = nn.W2\n",
    "b1 = nn.b1\n",
    "b2 = nn.b2\n",
    "\n",
    "# gradients par différence finie\n",
    "grad_w1_diff = np.zeros(W1.shape)\n",
    "grad_w2_diff = np.zeros(W2.shape)\n",
    "grad_b1_diff = np.zeros(b1.shape)\n",
    "grad_b2_diff = np.zeros(b2.shape)\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    grad_w1, grad_w2, grad_b1, grad_b2 = finite_diff(X[i,:], Y[i], nn, epsilon)\n",
    "    grad_w1_diff += grad_w1 / X.shape[0]\n",
    "    grad_w2_diff += grad_w2 / X.shape[0]\n",
    "    grad_b1_diff += grad_b1 / X.shape[0]\n",
    "    grad_b2_diff += grad_b2 / X.shape[0]\n",
    "    \n",
    "# gradients par implémentation dans classe neural_net    \n",
    "grad_W1, grad_W2, grad_b1, grad_b2, _ = nn.bprop(X,Y)\n",
    "\n",
    "# affichage de la différence\n",
    "print('Différence entre les deux gradients pour W1, W2, b1, b2:')\n",
    "#print(grad_w1_diff)\n",
    "#print(grad_W1)\n",
    "print(grad_W1 - grad_w1_diff)\n",
    "print(grad_W2 - grad_w2_diff)\n",
    "print(grad_b1 - grad_b1_diff)\n",
    "print(grad_b2 - grad_b2_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entraînement sur 2 moons et visualisation des régions de décision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction de calcul du taux d'erreur de classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cette fonction renvoie le taux d'erreur étant donné un classifieur et un ensemble de données\n",
    "def taux_erreur(classifieur, data):\n",
    "    x = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "    data_prob = classifieur.compute_predictions(x)\n",
    "    data_classe_pred = np.argmax(data_prob, axis=1)\n",
    "    \n",
    "    return 100. * np.mean(data_classe_pred != y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction d'affichage des régions de décision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fonction plot\n",
    "import pylab\n",
    "def gridplot(classifieur,train,test,n_points=50):\n",
    "\n",
    "    train_test = np.vstack((train,test))\n",
    "    (min_x1,max_x1) = (min(train_test[:,0]),max(train_test[:,0]))\n",
    "    (min_x2,max_x2) = (min(train_test[:,1]),max(train_test[:,1]))\n",
    "\n",
    "    xgrid = np.linspace(min_x1,max_x1,num=n_points)\n",
    "    ygrid = np.linspace(min_x2,max_x2,num=n_points)\n",
    "\n",
    "\t# calcule le produit cartesien entre deux listes\n",
    "    # et met les resultats dans un array\n",
    "    thegrid = np.array(combine(xgrid,ygrid))\n",
    "\n",
    "    les_comptes = classifieur.compute_predictions(thegrid)\n",
    "    classesPred = np.argmax(les_comptes,axis=1)+1\n",
    "\n",
    "    # La grille\n",
    "    pylab.pcolormesh(xgrid, ygrid, classesPred.reshape((n_points, n_points)).T, alpha=.3)\n",
    "\t# Les points d'entrainment\n",
    "    pylab.scatter(train[:,0], train[:,1], c = train[:,-1], marker = 'v', s=50)\n",
    "    # Les points de test\n",
    "    pylab.scatter(test[:,0], test[:,1], c = test[:,-1], marker = 's', s=50)\n",
    "\n",
    "    ## Un petit hack, parce que la fonctionalite manque a pylab...\n",
    "    h1, = pylab.plot([min_x1], [min_x2], marker='o', c = 'w',ms=5) \n",
    "    h2, = pylab.plot([min_x1], [min_x2], marker='v', c = 'w',ms=5) \n",
    "    h3, = pylab.plot([min_x1], [min_x2], marker='s', c = 'w',ms=5) \n",
    "    handles = [h1,h2,h3]\n",
    "    ## fin du hack\n",
    "\n",
    "    labels = ['grille','train','test']\n",
    "    pylab.legend(handles,labels)\n",
    "\n",
    "    pylab.axis('equal')\n",
    "    pylab.show()\n",
    "    \n",
    "## http://code.activestate.com/recipes/302478/\n",
    "def combine(*seqin):\n",
    "    '''returns a list of all combinations of argument sequences.\n",
    "for example: combine((1,2),(3,4)) returns\n",
    "[[1, 3], [1, 4], [2, 3], [2, 4]]'''\n",
    "    def rloop(seqin,listout,comb):\n",
    "        '''recursive looping function'''\n",
    "        if seqin:                       # any more sequences to process?\n",
    "            for item in seqin[0]:\n",
    "                newcomb=comb+[item]     # add next item to current comb\n",
    "                # call rloop w/ rem seqs, newcomb\n",
    "                rloop(seqin[1:],listout,newcomb)\n",
    "        else:                           # processing last sequence\n",
    "            listout.append(comb)        # comb finished, add to list\n",
    "    listout=[]                      # listout initialization\n",
    "    rloop(seqin,listout,[])         # start recursive process\n",
    "    return listout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entraînement sur les données 2 moons: définition d'ensembles de train et validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "ntrain = 2* (data.shape[0] // 3)\n",
    "\n",
    "inds = [i for i in range(data.shape[0])]\n",
    "np.random.seed(123)\n",
    "np.random.shuffle(inds)\n",
    "\n",
    "test_inds = inds[:ntrain]\n",
    "validation_inds = inds[ntrain:]\n",
    "\n",
    "#On définit l'ensemble d'entraînement et l'ensemble de validation\n",
    "train_data = data[test_inds,]\n",
    "validation_data = data[validation_inds,]\n",
    "print(train_data.shape)\n",
    "print(validation_data.shape)\n",
    "\n",
    "#définition de la fonction frange\n",
    "def frange(start, stop, step):\n",
    "    i = start\n",
    "    while i < stop:\n",
    "        yield i\n",
    "        i += step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On teste ici différentes combinaisons d'hyperparamètres contrôlant la capacité: nombres de neurones dans couche cachées (10, 20, 100 ou 200), valeurs des lambdas (0.1, 0.001, 0.00001, 0), et nombre d'époques (50, 100, 400, 1000). Nous affichons le taux d'erreur de classification pour chaque combinaison. Pour tester ces combinaisons, on fixe le learning rate à 0.05 et la taille des lots à 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Optimisation des paramètres de capacité: nbre couches cachées et lambdas\n",
    "#lambdas = np.array([[0.1, 0.1],\n",
    "#       [0.1, 0.1]])\n",
    "#lambdas = np.array([[0.001, 0.001],\n",
    "#       [0.001, 0.001]])\n",
    "lambdas = np.array([[0.00001, 0.00001],\n",
    "       [0.00001, 0.00001]])\n",
    "\n",
    "erreurs = np.zeros([4,4])\n",
    "i = 0\n",
    "for C in [10, 20, 100, 200]:\n",
    "    j = 0\n",
    "    for epoque in [50, 100, 400, 1000]:\n",
    "        nn = NeuralNet_loop(2,C,2,lambdas, 100)   \n",
    "        nn.train(train_data,epoque,0.05)  \n",
    "        erreurs[i,j] = taux_erreur(nn, validation_data)\n",
    "        j = j + 1\n",
    "    i = i + 1\n",
    "print(erreurs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que les lambdas doivent prendre des valeurs très petites pour que l'aprentissage fonctionne: à partir de 0.1 les lambdas sont déjà trop élevés et les $W_{ij}$ appris ont des valeurs trop petites. On obtient un taux d'erreur mauvais de 51.63%. Si on baisse la valeur des lambdas à 0.001 on peut obtenir des taux d'erreur jusqu'à 1.35%, et les meilleurs résultats sont obtenus en baissant lambda à 0.00001 ou à 0. Le meilleur taux d'erreur obtenu dans ces deux cas là est 0.81%\n",
    "\n",
    "On remarque qu'on peut obtenir des bon résultats, que ce soit avec 10, 20, 100 ou 200 neurones cachés. En revanche, plus le nombre de neurones dans la couche cachée est élevé, plus il faudra diminuer le nombre d'époques pour éviter l'overfitting.\n",
    "Avec 100 ou 200 neurones, le taux d'erreur est le meilleur après 400 itérations (0.81%), ensuite il remonte à 1.08%.\n",
    "\n",
    "Finalement, on constate que 50 ou 100 époques n'est pas suffisant pour diminuer le taux d'erreur (qui varie entre 9% et 20% selon le nombre de neurones dans la couche cachées). Nous avons besoin d'au moins 400 époques.\n",
    "\n",
    "Dans ce qui suit, on fixe le nombre de neurones dans la couche cachée à 20, on fixe lambda à 0.00001, et on optimise les paramètres d'optimisation. Nous testons différentes valeurs de learning rate, de taille de lot, et nous réoptimisons le nombre d'époques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zimmae\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:43: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "C:\\Users\\zimmae\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.35869565   1.08695652   1.08695652   1.08695652   1.08695652   0.        ]\n",
      " [  3.26086957   0.81521739   0.81521739   1.35869565   1.35869565   0.        ]\n",
      " [ 17.11956522   1.63043478   0.81521739   0.81521739   1.08695652   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Optimisation des paramètres d'optimisation: eta et K\n",
    "lambdas = np.array([[0.00001, 0.00001],\n",
    "       [0.00001, 0.00001]])\n",
    "\n",
    "epoque = 400\n",
    "#epoque = 1000\n",
    "erreurs = np.zeros([3,5])\n",
    "i = 0\n",
    "for K in [20, 100, 200]:\n",
    "    j = 0\n",
    "    for eta in frange(0.01,0.1,0.02):\n",
    "        nn = NeuralNet_loop(2,20,2,lambdas, K)   \n",
    "        nn.train(train_data,epoque,eta)  \n",
    "        erreurs[i,j] = taux_erreur(nn, validation_data)\n",
    "        j = j + 1\n",
    "    i = i + 1\n",
    "print(erreurs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En entraînant le modèle pendant 1000 époques, on constate que pour des mini-batches de taille 20, l'erreur descend même avec des valeurs de learning rates faibles, mais elle ne descend pas plus que 1.08%. En augmentant la taille de batch à 100 ou 200, on peut obtenir des taux d'erreurs à 0.81%, et dans ce cas l'algorithme est plus sensible au learning rate eta.\n",
    "\n",
    "Lorsque la taille de lot est 100, le meilleur taux d'erreur de 0.81% est obtenu avec eta entre 0.03 et 0.05. Lorsque la taille de lot est 200, le meilleur taux d'erreur est obtenu avec eta = 0.05. En revanche, pour des valeurs de eta plus faibles, le taux d'erreur remonte sensiblement. Si on prend beaucoup d'exemples par batch, il est donc crucial de ne pas choisir un learning rate trop faible.\n",
    "\n",
    "On observe aussi en diminuant le nombre d'époques, il faut augmenter un peu le learning rate pour garder un bon taux d'erreur. Par exemple, avec 400 époques, on obtient 0.81% avec eta = 0.07."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous affichons les régions de décisions pour plusieurs combinaisons d'hyperparamètres afin d'illustrer leur effet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperparamètres: 100 neurones cachés, lambdas à 0.00001, taille de lot 100, learning rate 0.05, 1000 époques\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zimmae\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:43: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "C:\\Users\\zimmae\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le taux d erreur est : 2.44565217391 %\n",
      "les régions de décisions sont:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf8AAAFkCAYAAAAuUDI+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4K+UUkolGQ3+SimlVJLR4K+UUkolmf8HbAVXPcY2HfgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xdc05f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''print('hyperparamètres: 20 neurones cachés, lambdas à 0.00001, taille de lot 100, learning rate 0.05, 1000 époques')\n",
    "lambdas = np.array([[0.0001, 0.0001],\n",
    "       [0.0001, 0.0001]])\n",
    "nn = NeuralNet_loop(2,20,2,lambdas, 100)   \n",
    "nn.train(train_data,1000,0.05) \n",
    "print('le taux d erreur est :', taux_erreur(nn, validation_data),'%')\n",
    "print('les régions de décisions sont:')\n",
    "gridplot(nn, train_data, validation_data, 50)\n",
    "\n",
    "print('hyperparamètres: 20 neurones cachés, lambdas à 0.1, taille de lot 100, learning rate 0.05, 1000 époques')\n",
    "lambdas = np.array([[0.1, 0.1],\n",
    "       [0.1, 0.1]])\n",
    "nn = NeuralNet_loop(2,20,2,lambdas, 100)   \n",
    "nn.train(train_data,1000,0.05) \n",
    "print('le taux d erreur est :', taux_erreur(nn, validation_data),'%')\n",
    "print('les régions de décisions sont:')\n",
    "gridplot(nn, train_data, validation_data, 50)\n",
    "\n",
    "print('hyperparamètres: 20 neurones cachés, lambdas à 0.00001, taille de lot 200, learning rate 0.01, 1000 époques')\n",
    "lambdas = np.array([[0.0001, 0.0001],\n",
    "       [0.0001, 0.0001]])\n",
    "nn = NeuralNet_loop(2,20,2,lambdas, 100)   \n",
    "nn.train(train_data,1000,0.01) \n",
    "print('le taux d erreur est :', taux_erreur(nn, validation_data),'%')\n",
    "print('les régions de décisions sont:')\n",
    "gridplot(nn, train_data, validation_data, 50)'''\n",
    "\n",
    "print('hyperparamètres: 100 neurones cachés, lambdas à 0.00001, taille de lot 100, learning rate 0.05, 1000 époques')\n",
    "lambdas = np.array([[0.0001, 0.0001],\n",
    "       [0.0001, 0.0001]])\n",
    "nn = NeuralNet_loop(2,100,2,lambdas, 100)   \n",
    "nn.train(train_data,1000,0.01) \n",
    "print('le taux d erreur est :', taux_erreur(nn, validation_data),'%')\n",
    "print('les régions de décisions sont:')\n",
    "gridplot(nn, train_data, validation_data, 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implémentation du calcul de gradient avec expressions matricielles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{W}^{(1)} \\in \\mathbb{R}^{d_h \\times d}$, $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ et $\\mathbf{B}^{(1)} \\in \\mathbb{R}^{d_h \\times n}$\n",
    "\n",
    "$$\\mathbf{h}^{a} = \\mathbf{W}^{(1)}\\mathbf{X}^{\\top} + \\mathbf{B}^{(1)} \\in \\mathbb{R}^{d_h \\times n}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetVectorized:\n",
    "    \n",
    "    def __init__(self, n_input, n_hidden, n_out, lambdas, K=1):\n",
    "        \n",
    "        self.n_in = n_input\n",
    "        self.n_h = n_hidden\n",
    "        self.n_o = n_out\n",
    "        self.lambdas = lambdas\n",
    "        self.K = K\n",
    "        \n",
    "        low_bound = -1 / np.sqrt([self.n_in, self.n_h])\n",
    "        up_bound = 1 / np.sqrt([self.n_in, self.n_h])\n",
    "        \n",
    "        # Initialize the parameters\n",
    "        np.random.seed(123)\n",
    "        self.W1 = np.random.uniform(low_bound[0], up_bound[0], size=(self.n_h, self.n_in))  # d_h x d\n",
    "        self.W2 = np.random.uniform(low_bound[1], up_bound[1], size=(self.n_o, self.n_h))  # m x d_h\n",
    "        self.b1 = np.zeros(self.n_h)  # dimension d_h\n",
    "        self.b2 = np.zeros(self.n_o) # dimension m\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        '''\n",
    "        Computes activations for every layer\n",
    "        X: input data set\n",
    "        '''\n",
    "        self.ha = self.W1.dot(X.T) + self.b1.reshape(self.n_h, 1)\n",
    "        self.hs = relu(self.ha)\n",
    "        self.oa = self.W2.dot(self.hs) + self.b2.reshape(self.n_o, 1)\n",
    "        self.os = np.transpose(softmax(self.oa.T))\n",
    "            \n",
    "    def bprop(self, X, Y):\n",
    "        '''\n",
    "        Computes the gradients, must be executed after fprop\n",
    "        X: Input data set\n",
    "        Y: targets\n",
    "        '''\n",
    "        n = X.shape[0]\n",
    "                      \n",
    "        grad_oa = self.os - onehot_matrix(self.n_o, Y)\n",
    "        grad_b2 = np.sum(grad_oa, axis =1) # m x n\n",
    "        grad_W2 = np.dot(grad_oa, self.hs.T) # sum of gradients grad_W2 for each example\n",
    "        grad_hs = self.W2.T.dot(grad_oa) # d_h x n\n",
    "        grad_ha = grad_hs * (self.ha > 0) # d_h x n\n",
    "        grad_W1 = np.dot(grad_ha, X) # sum of gradients grad_W1 for each example\n",
    "        grad_b1 = np.sum(grad_ha, axis =1) # d_h x n\n",
    "        \n",
    "        return grad_W1 / n, grad_W2 / n, grad_b1 / n, grad_b2 / n #returns average of the gradient\n",
    "    \n",
    "    def compute_loss(self, Y):\n",
    "        loss = 0\n",
    "        for i in range(Y.shape[0]):\n",
    "            loss = loss -np.log(self.os[Y[i],i])\n",
    "        return loss\n",
    "    \n",
    "    def train(self, train_data, max_iter, eta=0.05):\n",
    "        \n",
    "        n_batches = int(np.ceil(train_data.shape[0]/self.K)) # number of batches\n",
    "        \n",
    "        for i in range(max_iter):\n",
    "            for j in range(0,train_data.shape[0],self.K):\n",
    "                batch = train_data[j:j+self.K]\n",
    "                \n",
    "                self.fprop(batch[:,:-1])\n",
    "                grad_W1_mean, grad_W2_mean, grad_b1_mean, grad_b2_mean = self.bprop(batch[:,:-1], batch[:,-1]) \n",
    "                \n",
    "                n = len(batch)\n",
    "                \n",
    "                #regularization\n",
    "                penality_grad_W1 = self.lambdas[0][0] * np.sign(self.W1) + 2 * self.lambdas[0][1] * self.W1\n",
    "                penality_grad_W2 = self.lambdas[1][0] * np.sign(self.W2) + 2 * self.lambdas[1][1] * self.W2\n",
    "                \n",
    "                self.W1 = self.W1 - eta * (grad_W1_mean + penality_grad_W1)\n",
    "                self.W2 = self.W2 - eta * (grad_W2_mean + penality_grad_W2)\n",
    "                self.b1 = self.b1 - eta * grad_b1_mean\n",
    "                self.b2 = self.b2 - eta * grad_b2_mean\n",
    "                    \n",
    "    def compute_predictions(self, test_data):\n",
    "        self.fprop(test_data)\n",
    "        pred = self.os\n",
    "\n",
    "        return pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 7. Comparaison du gradient des deux implémentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zimmae\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:43: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "C:\\Users\\zimmae\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "C:\\Users\\zimmae\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:15: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.06451412 -0.06900175]\n",
      " [ 0.04133331 -0.04728092]]\n",
      "[[ 0.06451412 -0.06900175]\n",
      " [ 0.04133331 -0.04728092]]\n",
      "[[  1.38777878e-17   6.93889390e-17]\n",
      " [ -1.38777878e-17  -2.77555756e-17]]\n",
      "[[-0.10498392  0.0524078 ]\n",
      " [ 0.10498392 -0.0524078 ]]\n",
      "[[-0.10498392  0.0524078 ]\n",
      " [ 0.10498392 -0.0524078 ]]\n",
      "[[ -1.11022302e-16   1.38777878e-17]\n",
      " [  1.11022302e-16  -6.93889390e-18]]\n",
      "[ 0.05953552 -0.03744815]\n",
      "[ 0.05953552 -0.03744815]\n",
      "[  2.08166817e-17  -2.08166817e-17]\n",
      "[-0.06300307  0.06300307]\n",
      "[-0.06300307  0.06300307]\n",
      "[ -1.38777878e-17   1.38777878e-17]\n",
      "[[ 0.06451412 -0.06900175]\n",
      " [ 0.04133331 -0.04728092]]\n",
      "[[ 0.06451412 -0.06900175]\n",
      " [ 0.04133331 -0.04728092]]\n",
      "[[  1.38777878e-17   6.93889390e-17]\n",
      " [ -1.38777878e-17  -2.77555756e-17]]\n",
      "[[-0.10498392  0.0524078 ]\n",
      " [ 0.10498392 -0.0524078 ]]\n",
      "[[-0.10498392  0.0524078 ]\n",
      " [ 0.10498392 -0.0524078 ]]\n",
      "[[ -1.11022302e-16   1.38777878e-17]\n",
      " [  1.11022302e-16  -6.93889390e-18]]\n",
      "[ 0.05953552 -0.03744815]\n",
      "[ 0.05953552 -0.03744815]\n",
      "[  2.08166817e-17  -2.08166817e-17]\n",
      "[-0.06300307  0.06300307]\n",
      "[-0.06300307  0.06300307]\n",
      "[ -1.38777878e-17   1.38777878e-17]\n"
     ]
    }
   ],
   "source": [
    "#initialize a neural network of each implementation, with K=1 and K=10\n",
    "for K in range(1,11,9):\n",
    "    lambdas = np.array([[0, 0],\n",
    "           [0, 0]])\n",
    "    nn_loop = NeuralNet_loop(2,2,2,lambdas, K)\n",
    "    nn_matrix = NeuralNetVectorized(2,2,2,lambdas,K)\n",
    "    \n",
    "    #gradient with loop implementation\n",
    "    grad_W1_loop, grad_W2_loop, grad_b1_loop, grad_b2_loop, _ = nn_loop.bprop(train_data[:,:-1],train_data[:,-1])\n",
    "    \n",
    "    #gradient with matrix multiplications implementation\n",
    "    nn_matrix.fprop(train_data[:,:-1])\n",
    "    grad_W1_mat, grad_W2_mat, grad_b1_mat, grad_b2_mat = nn_matrix.bprop(train_data[:,:-1],train_data[:,-1])\n",
    "    \n",
    "    #print comparison of all gradients\n",
    "    print(grad_W1_loop)\n",
    "    print(grad_W1_mat)\n",
    "    print(grad_W1_loop - grad_W1_mat)\n",
    "\n",
    "    print(grad_W2_loop)\n",
    "    print(grad_W2_mat)\n",
    "    print(grad_W2_loop - grad_W2_mat)\n",
    "\n",
    "    print(grad_b1_loop)\n",
    "    print(grad_b1_mat)\n",
    "    print(grad_b1_loop - grad_b1_mat)\n",
    "\n",
    "    print(grad_b2_loop)\n",
    "    print(grad_b2_mat)\n",
    "    print(grad_b2_loop - grad_b2_mat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparaison du temps de calcul pour une époque quand K=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xa6812b0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAFfCAYAAACfj30KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x8cb7b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('mnist-original', data_home='/u/dift3395/sklearn_data')\n",
    "\n",
    "plt.imshow(mnist.data[21455,:].reshape(28, 28), interpolation=\"nearest\", cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diviser le dataset MNIST en ensembles train, valid et test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(50000, 785)\n",
      "(732, 3)\n"
     ]
    }
   ],
   "source": [
    "indices = np.random.permutation(len(mnist['target']))\n",
    "\n",
    "print(mnist['data'].shape)\n",
    "\n",
    "X_train = mnist['data'][indices][:50000]/255.\n",
    "X_valid = mnist['data'][indices][50000:60000]/255.\n",
    "X_test = mnist['data'][indices][60000:]/255.\n",
    "y_train = mnist['target'][indices][:50000]\n",
    "y_valid = mnist['target'][indices][50000:60000]\n",
    "y_test = mnist['target'][indices][60000:]\n",
    "\n",
    "#from keras.utils import np_utils\n",
    "#y_train = np_utils.to_categorical(y_train)\n",
    "#y_valid = np_utils.to_categorical(y_valid)\n",
    "#y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "#concatenate data and target into one np.array\n",
    "train = np.concatenate((X_train, np.reshape(y_train, (50000,1))), axis=1)\n",
    "valid = np.concatenate((X_valid, np.reshape(y_valid, (10000,1))), axis=1)\n",
    "test = np.concatenate((X_test, np.reshape(y_test, (10000,1))), axis=1)\n",
    "\n",
    "print(train.shape)\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zimmae\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:43: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "C:\\Users\\zimmae\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.834191084799\n",
      "--- 5.833999872207642 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zimmae\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:15: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.6530001163482666 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#initializing neural networks\n",
    "nn_loop = NeuralNet_loop(784,20,10,lambdas,100)\n",
    "nn_matrix = NeuralNetVectorized(784,20,10,lambdas,100)\n",
    "\n",
    "#train both neural networks\n",
    "import time\n",
    "start_time = time.time()\n",
    "nn_loop.train(train,1,0.05)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "nn_matrix.train(train,1,0.05)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Calcul de taux d'erreur de classification sur MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modification de la classe NeuralNetworkVectorized pour calculer les valeurs recherchées à chaque époque de la phase l'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NeuralNetVectorized2:\n",
    "    \n",
    "    def __init__(self, n_input, n_hidden, n_out, lambdas, K=1):\n",
    "        \n",
    "        self.n_in = n_input\n",
    "        self.n_h = n_hidden\n",
    "        self.n_o = n_out\n",
    "        self.lambdas = lambdas\n",
    "        self.K = K\n",
    "        \n",
    "        low_bound = -1 / np.sqrt([self.n_in, self.n_h])\n",
    "        up_bound = 1 / np.sqrt([self.n_in, self.n_h])\n",
    "        \n",
    "        # Initialize the parameters\n",
    "        np.random.seed(123)\n",
    "        self.W1 = np.random.uniform(low_bound[0], up_bound[0], size=(self.n_h, self.n_in))  # d_h x d\n",
    "        self.W2 = np.random.uniform(low_bound[1], up_bound[1], size=(self.n_o, self.n_h))  # m x d_h\n",
    "        self.b1 = np.zeros(self.n_h)  # dimension d_h\n",
    "        self.b2 = np.zeros(self.n_o) # dimension m\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        '''\n",
    "        Computes activations for every layer\n",
    "        X: input data set\n",
    "        '''\n",
    "        self.ha = self.W1.dot(X.T) + self.b1.reshape(self.n_h, 1)\n",
    "        self.hs = relu(self.ha)\n",
    "        self.oa = self.W2.dot(self.hs) + self.b2.reshape(self.n_o, 1)\n",
    "        self.os = np.transpose(softmax(self.oa.T))\n",
    "            \n",
    "    def bprop(self, X, Y):\n",
    "        '''\n",
    "        Computes the gradients, must be executed after fprop\n",
    "        X: Input data set\n",
    "        Y: targets\n",
    "        '''\n",
    "        n = X.shape[0]\n",
    "                      \n",
    "        grad_oa = self.os - onehot_matrix(self.n_o, Y)\n",
    "        grad_b2 = np.sum(grad_oa, axis =1) # m x n\n",
    "        grad_W2 = np.dot(grad_oa, self.hs.T) # sum of gradients grad_W2 for each example\n",
    "        grad_hs = self.W2.T.dot(grad_oa) # d_h x n\n",
    "        grad_ha = grad_hs * (self.ha > 0) # d_h x n\n",
    "        grad_W1 = np.dot(grad_ha, X) # sum of gradients grad_W1 for each example\n",
    "        grad_b1 = np.sum(grad_ha, axis =1) # d_h x n\n",
    "        \n",
    "        return grad_W1 / n, grad_W2 / n, grad_b1 / n, grad_b2 / n #returns average of the gradient\n",
    "    \n",
    "    def compute_loss(self, Y):\n",
    "        ind = [i for i in range(Y.shape[0])]\n",
    "        loss = self.os[Y.astype(int),ind]\n",
    "        return np.sum(-np.log(loss))\n",
    "    \n",
    "    def train(self, train_data, valid_data, test_data, max_iter, eta=0.05):\n",
    "        \n",
    "        n_batches = int(np.ceil(train_data.shape[0]/self.K)) # number of batches\n",
    "        \n",
    "        for i in range(max_iter):\n",
    "            for j in range(0,train_data.shape[0],self.K):\n",
    "                batch = train_data[j:j+self.K]\n",
    "                \n",
    "                self.fprop(batch[:,:-1])\n",
    "                grad_W1_mean, grad_W2_mean, grad_b1_mean, grad_b2_mean = self.bprop(batch[:,:-1], batch[:,-1]) \n",
    "                \n",
    "                n = len(batch)\n",
    "                \n",
    "                #regularization\n",
    "                penality_grad_W1 = self.lambdas[0][0] * np.sign(self.W1) + 2 * self.lambdas[0][1] * self.W1\n",
    "                penality_grad_W2 = self.lambdas[1][0] * np.sign(self.W2) + 2 * self.lambdas[1][1] * self.W2\n",
    "                \n",
    "                self.W1 = self.W1 - eta * (grad_W1_mean + penality_grad_W1)\n",
    "                self.W2 = self.W2 - eta * (grad_W2_mean + penality_grad_W2)\n",
    "                self.b1 = self.b1 - eta * grad_b1_mean\n",
    "                self.b2 = self.b2 - eta * grad_b2_mean\n",
    "                    \n",
    "            self.write_in_file(train_data, valid_data, test_data)\n",
    "                    \n",
    "    def compute_predictions(self, test_data):\n",
    "        self.fprop(test_data)\n",
    "        pred = self.os\n",
    "\n",
    "        return pred\n",
    "    \n",
    "    def write_in_file(self,train_data,valid_data,test_data):\n",
    "        train_prob = self.compute_predictions(train_data[:,:-1])\n",
    "        train_classe_pred = np.argmax(train_prob, axis = 0)  \n",
    "        valid_prob = self.compute_predictions(valid_data[:,:-1])\n",
    "        valid_classe_pred = np.argmax(valid_prob, axis = 0)\n",
    "        test_prob = self.compute_predictions(test_data[:,:-1])\n",
    "        test_classe_pred = np.argmax(test_prob, axis = 0)\n",
    "        classif_erreur_train = 100*float(sum(sum([train_classe_pred != train_data[:,-1]])))/train_data.shape[0]\n",
    "        classif_erreur_valid = 100*float(sum(sum([valid_classe_pred != valid_data[:,-1]])))/valid_data.shape[0]\n",
    "        classif_erreur_test = 100*float(sum(sum([test_classe_pred != test_data[:,-1]])))/test_data.shape[0]\n",
    "        self.fprop(train_data[:,:-1])\n",
    "        loss_train = self.compute_loss(train_data[:,-1])\n",
    "        print(classif_erreur_train)\n",
    "        print(classif_erreur_test)\n",
    "        print(classif_erreur_valid)\n",
    "        print(loss_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 3 7 2 2 6 7 0 0 9 7 3 8 0 6 8 8 8 8 3]\n"
     ]
    }
   ],
   "source": [
    "lambdas = np.array([[0.01, 0.01],\n",
    "           [0.01, 0.01]])\n",
    "nn_mnist = NeuralNetVectorized2(784,64,10,lambdas,train.shape[0])\n",
    "\n",
    "nn_mnist.fprop(train[:,:-1])\n",
    "#print(nn_mnist.ha)\n",
    "#print(nn_mnist.hs)\n",
    "#print(nn_mnist.oa)\n",
    "#print(nn_mnist.os)\n",
    "print(nn_mnist.compute_loss(train[:,-1]))\n",
    "nn_mnist.train(train,test,valid,10,0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
