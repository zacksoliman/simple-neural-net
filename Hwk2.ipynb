{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implémentation du réseau de neurones et expérimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Implémentation de fonctions utiles\n",
    "Calcul numériquement stable du softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_vector(x):\n",
    "    '''\n",
    "    x:  is a data vector.\n",
    "    returns: the result of the softmax function applied to the data vector.\n",
    "    '''\n",
    "    max_comp = np.amax(x)\n",
    "    normalized  = x - max_comp\n",
    "    \n",
    "    exponential = np.exp(normalized)\n",
    "    \n",
    "    return exponential/np.sum(exponential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    '''\n",
    "    X: matrix that holds the data, every row is a data vector.\n",
    "    returns: matrix where every row is the result of the softmax function applied to the corresponding data vector.\n",
    "    '''\n",
    "    \n",
    "    max_comp = np.amax(X, axis=1)\n",
    "    normalized  = X - max_comp.reshape(X.shape[0], 1)\n",
    "    \n",
    "    exponential = np.exp(normalized)\n",
    "    \n",
    "    return exponential/np.sum(exponential, axis=1).reshape(X.shape[0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction utilitaire pour calculer relu($x$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x, np.zeros(x.shape))\n",
    "\n",
    "def onehot(m, y):\n",
    "    return np.eye(m)[y]\n",
    "\n",
    "def onehot_matrix(m, targets):\n",
    "    \"\"\"\n",
    "    Returns: onehot matrix where every column is a onehot vector of the coressponding target\n",
    "    \"\"\"\n",
    "    eye = np.eye(m)\n",
    "    onehot_matrix = np.zeros((m,len(targets)))\n",
    "    \n",
    "    for i, y in enumerate(targets):\n",
    "        onehot_matrix[:,i] = eye[y]\n",
    "        \n",
    "    return onehot_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Calcul du gradient sur un exemple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation de fprop et bprop pour calculer le gradient sur un exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNet_basic:\n",
    "    \n",
    "    def __init__(self, n_input, n_hidden, n_out):\n",
    "        \n",
    "        self.n_in = n_input\n",
    "        self.n_h = n_hidden\n",
    "        self.n_o = n_out\n",
    "        \n",
    "        low_bound = -1 / np.sqrt([self.n_in, self.n_h])\n",
    "        up_bound = 1 / np.sqrt([self.n_in, self.n_h])\n",
    "        \n",
    "        # Initialize the parameters\n",
    "        self.W1 = np.random.uniform(low_bound[0], up_bound[0], size=(self.n_h, self.n_in))  # d_h x d\n",
    "        self.W2 = np.random.uniform(low_bound[1], up_bound[1], size=(self.n_o, self.n_h))  # m x d_h\n",
    "        self.b1 = np.zeros(self.n_h)  # dimension d_h\n",
    "        self.b2 = np.zeros(self.n_o) # dimension m\n",
    "    \n",
    "    def fprop(self, x):\n",
    "        '''Computes activations for every layer'''\n",
    "        self.ha = self.W1.dot(x) + self.b1\n",
    "        self.hs = relu(self.ha)\n",
    "        self.oa = self.W2.dot(self.hs) + self.b2\n",
    "        self.os = softmax_vector(self.oa)\n",
    "            \n",
    "    def bprop(self, x, y):\n",
    "        '''Computes the gradients, must be executed after fprop'''\n",
    "                      \n",
    "        grad_oa = self.os - onehot(self.n_o, y)\n",
    "        grad_b2 = grad_oa\n",
    "        grad_W2 = np.outer(grad_oa, self.hs)\n",
    "        grad_hs = self.W2.T.dot(grad_oa)\n",
    "        grad_ha = grad_hs * (self.ha > 0)\n",
    "        grad_W1 = np.outer(grad_ha, x)\n",
    "        grad_b1 = grad_ha\n",
    "        \n",
    "        return grad_W1, grad_W2, grad_b1, grad_b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vérification du gradient par différences finies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant vérifier que le gradient calculé est juste. Pour cela on va calculer une approximation du gradient en utilisant la méthode de la différence finie.\n",
    "\n",
    "Pour chaque composante de w1, on va:\n",
    " 1. calculer la valeur de la fonction objectif $L$\n",
    " 2. ajouter une petite valeur $\\epsilon$ à la composante\n",
    " 3. recalculer la valeur de la fonction objectif $L'$\n",
    " 4. remettre à l'ancienne valeur la composante (c'est à dire soustraire $\\epsilon$)\n",
    "\n",
    "Le gradient par différences finies sera dans ce cas donné par:\n",
    "$\\Big( \\frac{\\partial L}{\\partial W_{1}} \\Big)_{ij} = \\frac{1}{\\epsilon} (L' - L)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(x, y, W1, W2, b1, b2):\n",
    "    ha = W1.dot(x) + b1\n",
    "    hs = relu(ha)\n",
    "    oa = W2.dot(hs) + b2\n",
    "    os = softmax_vector(oa)\n",
    "    \n",
    "    return -np.log(os[y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction pour calculer le gradient par differences finies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def finite_diff(x, y, neural_net, eps=1e-5):\n",
    "    \n",
    "    # params\n",
    "    W1 = neural_net.W1\n",
    "    W2 = neural_net.W2\n",
    "    b1 = neural_net.b1\n",
    "    b2 = neural_net.b2\n",
    "    \n",
    "    # gradients\n",
    "    grad_w1_diff = np.zeros(W1.shape)\n",
    "    grad_w2_diff = np.zeros(W2.shape)\n",
    "    grad_b1_diff = np.zeros(b1.shape)\n",
    "    grad_b2_diff = np.zeros(b2.shape)\n",
    "    \n",
    "    neural_net.fprop(x)\n",
    "    loss = compute_loss(x, y, W1, W2, b1, b2)\n",
    "    \n",
    "    for i in range(W1.shape[0]):\n",
    "        for j in range(W1.shape[1]):\n",
    "            W1[i,j] = W1[i,j] + eps\n",
    "            loss_prime = compute_loss(x, y, W1, W2, b1, b2)\n",
    "            grad_w1_diff[i, j] = (loss_prime - loss) / epsilon\n",
    "            W1[i,j] = W1[i,j] - eps\n",
    "    for i in range(W2.shape[0]):\n",
    "        for j in range(W2.shape[1]):\n",
    "            W2[i,j] = W2[i,j] + eps\n",
    "            loss_prime = compute_loss(x, y, W1, W2, b1, b2)\n",
    "            grad_w2_diff[i, j] = (loss_prime - loss) / epsilon\n",
    "            W2[i,j] = W2[i,j] - eps\n",
    "    for i in range(b1.shape[0]):\n",
    "            b1[i] = b1[i] + eps\n",
    "            loss_prime = compute_loss(x, y, W1, W2, b1, b2)\n",
    "            grad_b1_diff[i] = (loss_prime - loss) / epsilon\n",
    "            b1[i] = b1[i] - eps\n",
    "    for i in range(b1.shape[0]):\n",
    "            b2[i] = b2[i] + eps\n",
    "            loss_prime = compute_loss(x, y, W1, W2, b1, b2)\n",
    "            grad_b2_diff[i] = (loss_prime - loss) / epsilon\n",
    "            b2[i] = b2[i] - eps\n",
    "            \n",
    "    return grad_w1_diff, grad_w2_diff, grad_b1_diff, grad_b2_diff\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage de vérification du gradient par différence finie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt(open('2moons.txt','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Différence entre les deux gradients pour W1, W2, b1, b2:\n",
      "[[-0.01039247  0.05107167]\n",
      " [ 0.          0.        ]]\n",
      "[[-0.01039247  0.05107166]\n",
      " [ 0.         -0.        ]]\n",
      "[[ 1.00000005  0.99999974]\n",
      " [        nan         nan]]\n",
      "[[ 1.00000067         nan]\n",
      " [ 0.99999933         nan]]\n",
      "[ 1.00000014         nan]\n",
      "[ 1.00000252  0.99999748]\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNet_basic(2, 2, 2)\n",
    "x = data[0,:-1]\n",
    "y = data[0,-1]\n",
    "epsilon = 1e-5\n",
    "\n",
    "# gradients par différence finie\n",
    "grad_w1_diff, grad_w2_diff, grad_b1_diff, grad_b2_diff = finite_diff(x, y, nn, epsilon)\n",
    "\n",
    "# gradients par implémentation dans classe neural_net\n",
    "grad_W1, grad_W2, grad_b1, grad_b2 = nn.bprop(x,y)\n",
    "\n",
    "# affichage de la différence\n",
    "print('Différence entre les deux gradients pour W1, W2, b1, b2:')\n",
    "print(grad_W1 / grad_w1_diff)\n",
    "print(grad_W2 / grad_w2_diff)\n",
    "print(grad_b1 / grad_b1_diff)\n",
    "print(grad_b2 / grad_b2_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque q'effectivement les ratio sont entre 0.99 et 1.01. Les NaN apparaissent lorsque les deux paramètres (ceux calculé avec la différence finie et le gradient) sont égales à zéro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient de W1 par difference finie:\n",
      "[[-0.01039247  0.05107167]\n",
      " [ 0.          0.        ]]\n",
      "Gradient de W1:\n",
      "[[-0.01039247  0.05107166]\n",
      " [ 0.         -0.        ]]\n",
      "Gradient de W2 par difference finie:\n",
      "[[-0.13105928  0.        ]\n",
      " [ 0.13105946  0.        ]]\n",
      "Gradient de W2:\n",
      "[[-0.13105937 -0.        ]\n",
      " [ 0.13105937  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Gradient de W1 par difference finie:\")\n",
    "print(grad_w1_diff)\n",
    "print(\"Gradient de W1:\")\n",
    "print(grad_W1)\n",
    "\n",
    "print(\"Gradient de W2 par difference finie:\")\n",
    "print(grad_w2_diff)\n",
    "print(\"Gradient de W2:\")\n",
    "print(grad_W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ajout de hyperparamètre de taille de lot K\n",
    "La fonction bprop est modifiée pour calculer le gradient sur un batch d'exemple. La fonction train est également ajoutée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NeuralNet_loop:\n",
    "    \n",
    "    def __init__(self, n_input, n_hidden, n_out, lambdas, K = 1):\n",
    "        \n",
    "        self.n_in = n_input\n",
    "        self.n_h = n_hidden\n",
    "        self.n_o = n_out\n",
    "        self.K = K\n",
    "        self.lambdas = lambdas\n",
    "        \n",
    "        low_bound = -1 / np.sqrt([self.n_in, self.n_h])\n",
    "        up_bound = 1 / np.sqrt([self.n_in, self.n_h])\n",
    "        \n",
    "        np.random.seed(123)\n",
    "        \n",
    "        # Initialize the parameters\n",
    "        self.W1 = np.random.uniform(low_bound[0], up_bound[0], size=(self.n_h, self.n_in))  # d_h x d\n",
    "        self.W2 = np.random.uniform(low_bound[1], up_bound[1], size=(self.n_o, self.n_h))  # m x d_h\n",
    "        self.b1 = np.zeros(self.n_h)  # dimension d_h\n",
    "        self.b2 = np.zeros(self.n_o) # dimension m\n",
    "    \n",
    "    def fprop(self, x):\n",
    "        '''Computes activations for every layer'''\n",
    "        ha = self.W1.dot(x) + self.b1\n",
    "        hs = relu(ha)\n",
    "        oa = self.W2.dot(hs) + self.b2\n",
    "        os = softmax_vector(oa)\n",
    "        return ha, hs, oa, os\n",
    "            \n",
    "    def bprop(self, X, Y):\n",
    "        '''Computes the gradients over all examples in (X,Y) with loop'''\n",
    "        grad_W1_mean = np.zeros((self.n_h, self.n_in))\n",
    "        grad_W2_mean = np.zeros((self.n_o, self.n_h))\n",
    "        grad_b1_mean = np.zeros(self.n_h)\n",
    "        grad_b2_mean = np.zeros(self.n_o)\n",
    "        \n",
    "        n = X.shape[0]\n",
    "        loss = 0.\n",
    "        \n",
    "        for i in range(n):\n",
    "            \n",
    "            ha, hs, oa, os = self.fprop(X[i,:])\n",
    "            loss += -np.log(os[Y[i]])\n",
    "            \n",
    "            grad_oa = os - onehot(self.n_o, Y[i])\n",
    "            grad_b2 = grad_oa\n",
    "            grad_W2 = np.outer(grad_oa, hs)\n",
    "            grad_hs = self.W2.T.dot(grad_oa)\n",
    "            grad_ha = grad_hs * (ha > 0)\n",
    "            grad_W1 = np.outer(grad_ha, X[i,:])\n",
    "            grad_b1 = grad_ha\n",
    "            \n",
    "            grad_W1_mean = grad_W1_mean + grad_W1 / n\n",
    "            grad_W2_mean = grad_W2_mean + grad_W2 / n\n",
    "            grad_b1_mean = grad_b1_mean + grad_b1 / n\n",
    "            grad_b2_mean = grad_b2_mean + grad_b2 / n\n",
    "        \n",
    "        return grad_W1_mean, grad_W2_mean, grad_b1_mean, grad_b2_mean, loss \n",
    "    \n",
    "    def compute_loss(self, x, y):\n",
    "        _, _, _, os = self.fprop(x)\n",
    "        return -np.log(self.os[y])\n",
    "    \n",
    "    def train(self, train_data, max_iter, eta=0.05):\n",
    "        \n",
    "        n_batches = int(np.ceil(train_data.shape[0]/self.K)) # number of batches\n",
    "        \n",
    "        # Initialize batch start and end indices\n",
    "        \n",
    "        for i in range(max_iter):\n",
    "            loss = 0.\n",
    "            for j in range(0,train_data.shape[0],self.K):\n",
    "                batch = train_data[j:j+self.K]\n",
    "                \n",
    "                grad_W1_mean, grad_W2_mean, grad_b1_mean, grad_b2_mean, bloss = self.bprop(batch[:,:-1], batch[:,-1]) \n",
    "                loss += bloss\n",
    "                \n",
    "                n = len(batch)\n",
    "                \n",
    "                #regularization\n",
    "                penality_grad_W1 = self.lambdas[0][0] * np.sign(self.W1) + 2 * self.lambdas[0][1] * self.W1\n",
    "                penality_grad_W2 = self.lambdas[1][0] * np.sign(self.W2) + 2 * self.lambdas[1][1] * self.W2\n",
    "                \n",
    "                self.W1 = self.W1 - eta * (grad_W1_mean + penality_grad_W1)\n",
    "                self.W2 = self.W2 - eta * (grad_W2_mean + penality_grad_W2)\n",
    "                self.b1 = self.b1 - eta * grad_b1_mean\n",
    "                self.b2 = self.b2 - eta * grad_b2_mean\n",
    "                \n",
    "            #if i%100 == 0:\n",
    "                #print('The loss after epoque ', i, ' is ', loss / train_data.shape[0])\n",
    "                    \n",
    "    def compute_predictions(self, test_data):\n",
    "\n",
    "        pred = np.empty((test_data.shape[0],self.n_o))\n",
    "\n",
    "        for i in range(test_data.shape[0]):\n",
    "            _, _, _, os = self.fprop(test_data[i,:])\n",
    "            pred[i,:] = os\n",
    "\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vérification du gradient par différence finie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage de vérification du gradient pour un lot de 10 exemples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Différence entre les deux gradients pour W1, W2, b1, b2:\n",
      "[[ 1.00000483  1.00000193]\n",
      " [ 0.9999974   1.00000191]]\n",
      "[[ 1.00000193  0.99999863]\n",
      " [ 0.99999807  1.00000138]]\n",
      "[ 0.99999848  1.00000208]\n",
      "[ 0.9999829  1.0000171]\n"
     ]
    }
   ],
   "source": [
    "lambdas = np.array([[0, 0],\n",
    "       [0, 0]])\n",
    "nn = NeuralNet_loop(2, 2, 2, lambdas)\n",
    "X = data[0:10,:-1]\n",
    "Y = data[0:10,-1]\n",
    "\n",
    "# params\n",
    "W1 = nn.W1\n",
    "W2 = nn.W2\n",
    "b1 = nn.b1\n",
    "b2 = nn.b2\n",
    "\n",
    "# gradients par différence finie\n",
    "grad_w1_diff = np.zeros(W1.shape)\n",
    "grad_w2_diff = np.zeros(W2.shape)\n",
    "grad_b1_diff = np.zeros(b1.shape)\n",
    "grad_b2_diff = np.zeros(b2.shape)\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    grad_w1, grad_w2, grad_b1, grad_b2 = finite_diff(X[i,:], Y[i], nn, epsilon)\n",
    "    grad_w1_diff += grad_w1 / X.shape[0]\n",
    "    grad_w2_diff += grad_w2 / X.shape[0]\n",
    "    grad_b1_diff += grad_b1 / X.shape[0]\n",
    "    grad_b2_diff += grad_b2 / X.shape[0]\n",
    "    \n",
    "# gradients par implémentation dans classe neural_net    \n",
    "grad_W1, grad_W2, grad_b1, grad_b2, _ = nn.bprop(X,Y)\n",
    "\n",
    "# affichage de la différence\n",
    "print('Différence entre les deux gradients pour W1, W2, b1, b2:')\n",
    "\n",
    "print(grad_W1 / grad_w1_diff)\n",
    "print(grad_W2 / grad_w2_diff)\n",
    "print(grad_b1 / grad_b1_diff)\n",
    "print(grad_b2 / grad_b2_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que les ration sont effectivement entre 0.99 et 1.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entraînement sur 2 moons et visualisation des régions de décision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction de calcul du taux d'erreur de classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cette fonction renvoie le taux d'erreur étant donné un classifieur et un ensemble de données\n",
    "def taux_erreur(classifieur, data):\n",
    "    x = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "    data_prob = classifieur.compute_predictions(x)\n",
    "    data_classe_pred = np.argmax(data_prob, axis=1)\n",
    "    \n",
    "    return 100. * np.mean(data_classe_pred != y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction d'affichage des régions de décision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fonction plot\n",
    "import pylab\n",
    "def gridplot(classifieur,train,test,n_points=50):\n",
    "\n",
    "    train_test = np.vstack((train,test))\n",
    "    (min_x1,max_x1) = (min(train_test[:,0]),max(train_test[:,0]))\n",
    "    (min_x2,max_x2) = (min(train_test[:,1]),max(train_test[:,1]))\n",
    "\n",
    "    xgrid = np.linspace(min_x1,max_x1,num=n_points)\n",
    "    ygrid = np.linspace(min_x2,max_x2,num=n_points)\n",
    "\n",
    "\t# calcule le produit cartesien entre deux listes\n",
    "    # et met les resultats dans un array\n",
    "    thegrid = np.array(combine(xgrid,ygrid))\n",
    "\n",
    "    les_comptes = classifieur.compute_predictions(thegrid)\n",
    "    classesPred = np.argmax(les_comptes,axis=1)+1\n",
    "\n",
    "    # La grille\n",
    "    pylab.pcolormesh(xgrid, ygrid, classesPred.reshape((n_points, n_points)).T, alpha=.3)\n",
    "\t# Les points d'entrainment\n",
    "    pylab.scatter(train[:,0], train[:,1], c = train[:,-1], marker = 'v', s=50)\n",
    "    # Les points de test\n",
    "    pylab.scatter(test[:,0], test[:,1], c = test[:,-1], marker = 's', s=50)\n",
    "\n",
    "    ## Un petit hack, parce que la fonctionalite manque a pylab...\n",
    "    h1, = pylab.plot([min_x1], [min_x2], marker='o', c = 'w',ms=5) \n",
    "    h2, = pylab.plot([min_x1], [min_x2], marker='v', c = 'w',ms=5) \n",
    "    h3, = pylab.plot([min_x1], [min_x2], marker='s', c = 'w',ms=5) \n",
    "    handles = [h1,h2,h3]\n",
    "    ## fin du hack\n",
    "\n",
    "    labels = ['grille','train','test']\n",
    "    pylab.legend(handles,labels)\n",
    "\n",
    "    pylab.axis('equal')\n",
    "    pylab.show()\n",
    "    \n",
    "## http://code.activestate.com/recipes/302478/\n",
    "def combine(*seqin):\n",
    "    '''returns a list of all combinations of argument sequences.\n",
    "for example: combine((1,2),(3,4)) returns\n",
    "[[1, 3], [1, 4], [2, 3], [2, 4]]'''\n",
    "    def rloop(seqin,listout,comb):\n",
    "        '''recursive looping function'''\n",
    "        if seqin:                       # any more sequences to process?\n",
    "            for item in seqin[0]:\n",
    "                newcomb=comb+[item]     # add next item to current comb\n",
    "                # call rloop w/ rem seqs, newcomb\n",
    "                rloop(seqin[1:],listout,newcomb)\n",
    "        else:                           # processing last sequence\n",
    "            listout.append(comb)        # comb finished, add to list\n",
    "    listout=[]                      # listout initialization\n",
    "    rloop(seqin,listout,[])         # start recursive process\n",
    "    return listout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entraînement sur les données 2 moons: définition d'ensembles de train et validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(732, 3)\n",
      "(368, 3)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "ntrain = 2* (data.shape[0] // 3)\n",
    "\n",
    "inds = [i for i in range(data.shape[0])]\n",
    "np.random.seed(123)\n",
    "np.random.shuffle(inds)\n",
    "\n",
    "test_inds = inds[:ntrain]\n",
    "validation_inds = inds[ntrain:]\n",
    "\n",
    "#On définit l'ensemble d'entraînement et l'ensemble de validation\n",
    "train_data = data[test_inds,]\n",
    "validation_data = data[validation_inds,]\n",
    "print(train_data.shape)\n",
    "print(validation_data.shape)\n",
    "\n",
    "#définition de la fonction frange\n",
    "def frange(start, stop, step):\n",
    "    i = start\n",
    "    while i < stop:\n",
    "        yield i\n",
    "        i += step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On teste ici différentes combinaisons d'hyperparamètres contrôlant la capacité: nombres de neurones dans couche cachées (10, 20, 100 ou 200), valeurs des lambdas (0.1, 0.001, 0.00001, 0), et nombre d'époques (50, 100, 400, 1000). Nous affichons le taux d'erreur de classification pour chaque combinaison. Pour tester ces combinaisons, on fixe le learning rate à 0.05 et la taille des lots à 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-e76b9b7906ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoque\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNet_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambdas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoque\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0merreurs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtaux_erreur\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-a5db753c189d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_data, max_iter, eta)\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0mgrad_W1_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_W2_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_b1_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_b2_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-a5db753c189d>\u001b[0m in \u001b[0;36mbprop\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mgrad_W1_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_W1_mean\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgrad_W1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mgrad_W2_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_W2_mean\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgrad_W2\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mgrad_b1_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_b1_mean\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgrad_b1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mgrad_b2_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_b2_mean\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgrad_b2\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Optimisation des paramètres de capacité: nbre couches cachées et lambdas\n",
    "#lambdas = np.array([[0.1, 0.1],\n",
    "#       [0.1, 0.1]])\n",
    "#lambdas = np.array([[0.001, 0.001],\n",
    "#       [0.001, 0.001]])\n",
    "lambdas = np.array([[0.00001, 0.00001],\n",
    "       [0.00001, 0.00001]])\n",
    "\n",
    "erreurs = np.zeros([4,4])\n",
    "i = 0\n",
    "for C in [10, 20, 100, 200]:\n",
    "    j = 0\n",
    "    for epoque in [50, 100, 400, 1000]:\n",
    "        nn = NeuralNet_loop(2,C,2,lambdas, 100)   \n",
    "        nn.train(train_data,epoque,0.05)  \n",
    "        erreurs[i,j] = taux_erreur(nn, validation_data)\n",
    "        j = j + 1\n",
    "    i = i + 1\n",
    "print(erreurs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que les lambdas doivent prendre des valeurs très petites pour que l'aprentissage fonctionne: à partir de 0.1 les lambdas sont déjà trop élevés et les $W_{ij}$ appris ont des valeurs trop petites. On obtient un taux d'erreur mauvais de 51.63%. Si on baisse la valeur des lambdas à 0.001 on peut obtenir des taux d'erreur jusqu'à 1.35%, et les meilleurs résultats sont obtenus en baissant lambda à 0.00001 ou à 0. Le meilleur taux d'erreur obtenu dans ces deux cas là est 0.81%\n",
    "\n",
    "On remarque qu'on peut obtenir des bon résultats, que ce soit avec 10, 20, 100 ou 200 neurones cachés. En revanche, plus le nombre de neurones dans la couche cachée est élevé, plus il faudra diminuer le nombre d'époques pour éviter l'overfitting.\n",
    "Avec 100 ou 200 neurones, le taux d'erreur est le meilleur après 400 itérations (0.81%), ensuite il remonte à 1.08%.\n",
    "\n",
    "Finalement, on constate que 50 ou 100 époques n'est pas suffisant pour diminuer le taux d'erreur (qui varie entre 9% et 20% selon le nombre de neurones dans la couche cachées). Nous avons besoin d'au moins 400 époques.\n",
    "\n",
    "Dans ce qui suit, on fixe le nombre de neurones dans la couche cachée à 20, on fixe lambda à 0.00001, et on optimise les paramètres d'optimisation. Nous testons différentes valeurs de learning rate, de taille de lot, et nous réoptimisons le nombre d'époques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Optimisation des paramètres d'optimisation: eta et K\n",
    "lambdas = np.array([[0.00001, 0.00001],\n",
    "       [0.00001, 0.00001]])\n",
    "\n",
    "epoque = 400\n",
    "#epoque = 1000\n",
    "erreurs = np.zeros([3,5])\n",
    "i = 0\n",
    "for K in [20, 100, 200]:\n",
    "    j = 0\n",
    "    for eta in frange(0.01,0.1,0.02):\n",
    "        nn = NeuralNet_loop(2,20,2,lambdas, K)   \n",
    "        nn.train(train_data,epoque,eta)  \n",
    "        erreurs[i,j] = taux_erreur(nn, validation_data)\n",
    "        j = j + 1\n",
    "    i = i + 1\n",
    "print(erreurs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En entraînant le modèle pendant 1000 époques, on constate que pour des mini-batches de taille 20, l'erreur descend même avec des valeurs de learning rates faibles, mais elle ne descend pas plus que 1.08%. En augmentant la taille de batch à 100 ou 200, on peut obtenir des taux d'erreurs à 0.81%, et dans ce cas l'algorithme est plus sensible au learning rate eta.\n",
    "\n",
    "Lorsque la taille de lot est 100, le meilleur taux d'erreur de 0.81% est obtenu avec eta entre 0.03 et 0.05. Lorsque la taille de lot est 200, le meilleur taux d'erreur est obtenu avec eta = 0.05. En revanche, pour des valeurs de eta plus faibles, le taux d'erreur remonte sensiblement. Si on prend beaucoup d'exemples par batch, il est donc crucial de ne pas choisir un learning rate trop faible.\n",
    "\n",
    "On observe aussi en diminuant le nombre d'époques, il faut augmenter un peu le learning rate pour garder un bon taux d'erreur. Par exemple, avec 400 époques, on obtient 0.81% avec eta = 0.07."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous affichons les régions de décisions pour plusieurs combinaisons d'hyperparamètres afin d'illustrer leur effet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Bonne combinaison d hyperparamètres')\n",
    "print('hyperparamètres: 20 neurones cachés, lambdas à 0.00001, taille de lot 100, learning rate 0.05, 1000 époques')\n",
    "lambdas = np.array([[0.0001, 0.0001],\n",
    "       [0.0001, 0.0001]])\n",
    "nn = NeuralNet_loop(2,20,2,lambdas, 100)   \n",
    "nn.train(train_data,1000,0.05) \n",
    "print('le taux d erreur est :', taux_erreur(nn, validation_data),'%')\n",
    "print('les régions de décisions sont:')\n",
    "gridplot(nn, train_data, validation_data, 50)\n",
    "\n",
    "print('Valeur de lambdas trop élevée')\n",
    "print('hyperparamètres: 20 neurones cachés, lambdas à 0.1, taille de lot 100, learning rate 0.05, 1000 époques')\n",
    "lambdas = np.array([[0.1, 0.1],\n",
    "       [0.1, 0.1]])\n",
    "nn = NeuralNet_loop(2,20,2,lambdas, 100)   \n",
    "nn.train(train_data,1000,0.05) \n",
    "print('le taux d erreur est :', taux_erreur(nn, validation_data),'%')\n",
    "print('les régions de décisions sont:')\n",
    "gridplot(nn, train_data, validation_data, 50)\n",
    "\n",
    "print('Taille de lot trop élevée pour learning rate trop faible')\n",
    "print('hyperparamètres: 20 neurones cachés, lambdas à 0.00001, taille de lot 200, learning rate 0.01, 1000 époques')\n",
    "lambdas = np.array([[0.0001, 0.0001],\n",
    "       [0.0001, 0.0001]])\n",
    "nn = NeuralNet_loop(2,20,2,lambdas, 100)   \n",
    "nn.train(train_data,1000,0.01) \n",
    "print('le taux d erreur est :', taux_erreur(nn, validation_data),'%')\n",
    "print('les régions de décisions sont:')\n",
    "gridplot(nn, train_data, validation_data, 50)\n",
    "\n",
    "print('Nombre de neurones dans couche cachées trop élevé amenant à léger sur-apprentissage')\n",
    "print('hyperparamètres: 100 neurones cachés, lambdas à 0.00001, taille de lot 100, learning rate 0.05, 1000 époques')\n",
    "lambdas = np.array([[0.0001, 0.0001],\n",
    "       [0.0001, 0.0001]])\n",
    "nn = NeuralNet_loop(2,100,2,lambdas, 100)   \n",
    "nn.train(train_data,1000,0.05) \n",
    "print('le taux d erreur est :', taux_erreur(nn, validation_data),'%')\n",
    "print('les régions de décisions sont:')\n",
    "gridplot(nn, train_data, validation_data, 50)\n",
    "\n",
    "print('Nombre d époques trop faible, arrêt trop prématuré de l apprentissage')\n",
    "print('hyperparamètres: 100 neurones cachés, lambdas à 0.00001, taille de lot 100, learning rate 0.05, 50 époques')\n",
    "lambdas = np.array([[0.0001, 0.0001],\n",
    "       [0.0001, 0.0001]])\n",
    "nn = NeuralNet_loop(2,100,2,lambdas, 100)   \n",
    "nn.train(train_data,50,0.05) \n",
    "print('le taux d erreur est :', taux_erreur(nn, validation_data),'%')\n",
    "print('les régions de décisions sont:')\n",
    "gridplot(nn, train_data, validation_data, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implémentation du calcul de gradient avec expressions matricielles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, on va représenter nos données par une matrice $\\mathbf{X}$ de dimension $n \\times d$  où chaque ligne de la matrice est un exemple $\\mathbf{x}^{(i)}$. On définit une matrice $\\mathbf{B}^{(1)}$ où chaque colonne est une copie du vecteur $\\mathbf{b}^{(1)}$. On applique donc l'équation suivante pour calculer $\\mathbf{h}^{a}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{W}^{(1)} \\in \\mathbb{R}^{d_h \\times d}, \\;\\; \\mathbf{X} \\in \\mathbb{R}^{n \\times d} \\;$ et $\\;\\mathbf{B}^{(1)} \\in \\mathbb{R}^{d_h \\times n}$\n",
    "\n",
    "$$\\mathbf{h}^{a} = \\mathbf{W}^{(1)}\\mathbf{X}^{\\top} + \\mathbf{B}^{(1)} \\in \\mathbb{R}^{d_h \\times n}$$\n",
    "\n",
    "Remarquez qu'avec numpy nous n'avons pas besoin de recopier le vecteur $\\mathbf{b}^{(1)}$ n fois. Nous pouvons simplement faire la somme de chaque colonnes de $\\mathbf{X}$ avec le vecteur colonne $\\mathbf{b}^{(1)}$:\n",
    "\n",
    "```self.W1.dot(X.T) + self.b1.reshape(self.n_h, 1)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alors $\\mathbf{h}^{a}$ sera une matrice de dimension $d_h \\times n$ où chaque colonne $j$ est le vecteur de preactivation des neuronnes de la couche cachée pour le $j^{ieme}$ exemple. Ensuite, pour le calcul, $\\mathbf{h}^s$ on applique la fonction relu$(\\mathbf{h}^{a})$ qui retourne une matrice  de dimension $d_h \\times n$ où la colonne $j$ est le vecteur de sortie des neuronnes de la couche cachée qui correspond au $j^{ieme}$ exemple d'entraînement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Pour le calcule de $\\mathbf{o}^a$, on applique le même principe:\n",
    "\n",
    "$\\mathbf{W}^{(2)} \\in \\mathbb{R}^{m \\times d_h}, \\;\\; \\mathbf{h}^{s} \\in \\mathbb{R}^{d_h \\times n} \\;$ et $\\;\\mathbf{B}^{(2)} \\in \\mathbb{R}^{m \\times n}$\n",
    "\n",
    "$$\\mathbf{o}^{a} = \\mathbf{W}^{(2)}\\mathbf{h}^{s} + \\mathbf{B}^{(2)} \\in \\mathbb{R}^{m \\times n}$$\n",
    "et,\n",
    "$$\\mathbf{o}^{s} = \\text{softmax}(\\mathbf{o}^{a}) \\in \\mathbb{R}^{m \\times n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "où la colonne $j$ de $\\mathbf{o}^a$ est le vecteur de preactivation de la couche cachee pour le $j^{ieme}$ exemple de notre ensemble d'entraînement. Pour la matrice $\\mathbf{o}^{s}$, chaque colonne correspond au vecteur de sortie pour chaque exemple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le calcul des gradient, on a:\n",
    "$$\\mathbf{Y} \\in \\mathbb{R}^{n}$$ est un vecteur où chaque composante est la classes de l'exemple d'entraînement coresspondant. On redéfinit la fonction onehot pour l'appliquer sur le vecteur $\\mathbf{Y}$. Cette nouvelle fonction va retourner une matrice ou chaque colonne est le vecteur onehot qui correspond chaque composante.\n",
    "\n",
    "$$\\nabla_{\\mathbf{o}^a} = \\mathbf{o}^s - \\text{onehot}(\\mathbf{Y}) \\in \\mathbb{R}^{m \\times n}$$\n",
    "\n",
    "\n",
    "$$\\nabla_{\\mathbf{B}^{(2)}} = \\nabla_{\\mathbf{o}^a} \\in \\mathbb{R}^{m \\times n}$$\n",
    "\n",
    "\n",
    "$$\\sum_{\\mathbf{W}^{(2)}} = \\nabla_{\\mathbf{W}^{(2)}} = \\mathbf{W}^{(2)^{\\top}} \\nabla_{\\mathbf{o}^a} \\in \\mathbb{R}^{m \\times n}$$\n",
    "\n",
    "Correspond a la sommes des gradients par rapport a $\\mathbf{W}^{(2)}$ sur tout l'ensemble d'entraînement\n",
    "\n",
    "\n",
    "$$\\sum_{\\mathbf{W}^{(1)}} = \\nabla_{\\mathbf{W}^{(1)}} = \\mathbf{h}^{a} \\mathbf{X} \\in \\mathbb{R}^{d_h \\times n}$$\n",
    "\n",
    "\n",
    "$$\\nabla_{\\mathbf{B}^{(1)}} = \\nabla_{\\mathbf{h}^a} \\in \\mathbb{R}^{d_h \\times n}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetVectorized:\n",
    "    \n",
    "    def __init__(self, n_input, n_hidden, n_out, lambdas, K=1):\n",
    "        \n",
    "        self.n_in = n_input\n",
    "        self.n_h = n_hidden\n",
    "        self.n_o = n_out\n",
    "        self.lambdas = lambdas\n",
    "        self.K = K\n",
    "        \n",
    "        low_bound = -1 / np.sqrt([self.n_in, self.n_h])\n",
    "        up_bound = 1 / np.sqrt([self.n_in, self.n_h])\n",
    "        \n",
    "        # Initialize the parameters\n",
    "        np.random.seed(123)\n",
    "        self.W1 = np.random.uniform(low_bound[0], up_bound[0], size=(self.n_h, self.n_in))  # d_h x d\n",
    "        self.W2 = np.random.uniform(low_bound[1], up_bound[1], size=(self.n_o, self.n_h))  # m x d_h\n",
    "        self.b1 = np.zeros(self.n_h)  # dimension d_h\n",
    "        self.b2 = np.zeros(self.n_o) # dimension m\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        '''\n",
    "        Computes activations for every layer\n",
    "        X: input data set\n",
    "        '''\n",
    "        self.ha = self.W1.dot(X.T) + self.b1.reshape(self.n_h, 1)\n",
    "        self.hs = relu(self.ha)\n",
    "        self.oa = self.W2.dot(self.hs) + self.b2.reshape(self.n_o, 1)\n",
    "        self.os = np.transpose(softmax(self.oa.T))\n",
    "            \n",
    "    def bprop(self, X, Y):\n",
    "        '''\n",
    "        Computes the gradients, must be executed after fprop\n",
    "        X: Input data set\n",
    "        Y: targets\n",
    "        '''\n",
    "        n = X.shape[0]\n",
    "                      \n",
    "        grad_oa = self.os - onehot_matrix(self.n_o, Y)\n",
    "        grad_b2 = np.sum(grad_oa, axis =1) # sum of b2 vectors on all training examples (m x 1)\n",
    "        grad_W2 = np.dot(grad_oa, self.hs.T) # sum of gradients grad_W2 for each example\n",
    "        grad_hs = self.W2.T.dot(grad_oa) # d_h x n\n",
    "        grad_ha = grad_hs * (self.ha > 0) # d_h x n\n",
    "        grad_W1 = np.dot(grad_ha, X) # sum of gradients grad_W1 for each example\n",
    "        grad_b1 = np.sum(grad_ha, axis =1) # sum of b1 vectors on all training examples (d_h x 1)\n",
    "        \n",
    "        return grad_W1 / n, grad_W2 / n, grad_b1 / n, grad_b2 / n #returns average of the gradient\n",
    "    \n",
    "    def compute_loss(self, Y):\n",
    "        loss = 0\n",
    "        for i in range(Y.shape[0]):\n",
    "            loss = loss -np.log(self.os[Y[i],i])\n",
    "        return loss\n",
    "    \n",
    "    def train(self, train_data, max_iter, eta=0.05):\n",
    "        \n",
    "        n_batches = int(np.ceil(train_data.shape[0]/self.K)) # number of batches\n",
    "        \n",
    "        for i in range(max_iter):\n",
    "            for j in range(0,train_data.shape[0],self.K):\n",
    "                batch = train_data[j:j+self.K]\n",
    "                \n",
    "                self.fprop(batch[:,:-1])\n",
    "                grad_W1_mean, grad_W2_mean, grad_b1_mean, grad_b2_mean = self.bprop(batch[:,:-1], batch[:,-1]) \n",
    "                \n",
    "                n = len(batch)\n",
    "                \n",
    "                #regularization\n",
    "                penality_grad_W1 = self.lambdas[0][0] * np.sign(self.W1) + 2 * self.lambdas[0][1] * self.W1\n",
    "                penality_grad_W2 = self.lambdas[1][0] * np.sign(self.W2) + 2 * self.lambdas[1][1] * self.W2\n",
    "                \n",
    "                self.W1 = self.W1 - eta * (grad_W1_mean + penality_grad_W1)\n",
    "                self.W2 = self.W2 - eta * (grad_W2_mean + penality_grad_W2)\n",
    "                self.b1 = self.b1 - eta * grad_b1_mean\n",
    "                self.b2 = self.b2 - eta * grad_b2_mean\n",
    "                    \n",
    "    def compute_predictions(self, test_data):\n",
    "        self.fprop(test_data)\n",
    "        pred = self.os\n",
    "\n",
    "        return pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 7. Comparaison du gradient des deux implémentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Différence entre les deux gradients pour W1, W2, b1, b2 lorsque la taille du lot est  1\n",
      "[[ 1.  1.]\n",
      " [ 1.  1.]]\n",
      "[[ 1.  1.]\n",
      " [ 1.  1.]]\n",
      "[ 1.  1.]\n",
      "[ 1.  1.]\n",
      "Différence entre les deux gradients pour W1, W2, b1, b2 lorsque la taille du lot est  10\n",
      "[[ 1.  1.]\n",
      " [ 1.  1.]]\n",
      "[[ 1.  1.]\n",
      " [ 1.  1.]]\n",
      "[ 1.  1.]\n",
      "[ 1.  1.]\n"
     ]
    }
   ],
   "source": [
    "#initialize a neural network of each implementation, with K=1 and K=10\n",
    "for K in range(1,11,9):\n",
    "    lambdas = np.array([[0, 0],\n",
    "           [0, 0]])\n",
    "    nn_loop = NeuralNet_loop(2,2,2,lambdas, K)\n",
    "    nn_matrix = NeuralNetVectorized(2,2,2,lambdas,K)\n",
    "    \n",
    "    #gradient with loop implementation\n",
    "    grad_W1_loop, grad_W2_loop, grad_b1_loop, grad_b2_loop, _ = nn_loop.bprop(train_data[:,:-1],train_data[:,-1])\n",
    "    \n",
    "    #gradient with matrix multiplications implementation\n",
    "    nn_matrix.fprop(train_data[:,:-1])\n",
    "    grad_W1_mat, grad_W2_mat, grad_b1_mat, grad_b2_mat = nn_matrix.bprop(train_data[:,:-1],train_data[:,-1])\n",
    "    \n",
    "    #print comparison of all gradients\n",
    "    print('Différence entre les deux gradients pour W1, W2, b1, b2 lorsque la taille du lot est ', K)\n",
    "    print(grad_W1_loop / grad_W1_mat)\n",
    "\n",
    "    print(grad_W2_loop / grad_W2_mat)\n",
    "\n",
    "    print(grad_b1_loop / grad_b1_mat)\n",
    "\n",
    "    print(grad_b2_loop / grad_b2_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remaque que les parametre calculé par descente de gradient avec les deux modèles on un ratio de 1 (i.e.: ils sont egale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparaison du temps de calcul pour une époque quand K=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('mnist-original', data_home='/u/dift3395/sklearn_data')\n",
    "\n",
    "plt.imshow(mnist.data[21455,:].reshape(28, 28), interpolation=\"nearest\", cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diviser le dataset MNIST en ensembles train, valid et test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(50000, 785)\n",
      "(732, 3)\n"
     ]
    }
   ],
   "source": [
    "indices = np.random.permutation(len(mnist['target']))\n",
    "\n",
    "print(mnist['data'].shape)\n",
    "\n",
    "X_train = mnist['data'][indices][:50000]/255.\n",
    "X_valid = mnist['data'][indices][50000:60000]/255.\n",
    "X_test = mnist['data'][indices][60000:]/255.\n",
    "y_train = mnist['target'][indices][:50000]\n",
    "y_valid = mnist['target'][indices][50000:60000]\n",
    "y_test = mnist['target'][indices][60000:]\n",
    "\n",
    "#from keras.utils import np_utils\n",
    "#y_train = np_utils.to_categorical(y_train)\n",
    "#y_valid = np_utils.to_categorical(y_valid)\n",
    "#y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "#concatenate data and target into one np.array\n",
    "train = np.concatenate((X_train, np.reshape(y_train, (50000,1))), axis=1)\n",
    "valid = np.concatenate((X_valid, np.reshape(y_valid, (10000,1))), axis=1)\n",
    "test = np.concatenate((X_test, np.reshape(y_test, (10000,1))), axis=1)\n",
    "\n",
    "print(train.shape)\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zimmae\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:43: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "C:\\Users\\zimmae\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.834191084799\n",
      "--- 5.833999872207642 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zimmae\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:15: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.6530001163482666 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#initializing neural networks\n",
    "nn_loop = NeuralNet_loop(784,20,10,lambdas,100)\n",
    "nn_matrix = NeuralNetVectorized(784,20,10,lambdas,100)\n",
    "\n",
    "#train both neural networks\n",
    "import time\n",
    "start_time = time.time()\n",
    "nn_loop.train(train,1,0.05)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "nn_matrix.train(train,1,0.05)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Calcul de taux d'erreur de classification sur MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modification de la classe NeuralNetworkVectorized pour calculer les valeurs recherchées à chaque époque de la phase l'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NeuralNetVectorized2:\n",
    "    \n",
    "    def __init__(self, n_input, n_hidden, n_out, lambdas, K=1):\n",
    "        \n",
    "        self.n_in = n_input\n",
    "        self.n_h = n_hidden\n",
    "        self.n_o = n_out\n",
    "        self.lambdas = lambdas\n",
    "        self.K = K\n",
    "        \n",
    "        low_bound = -1 / np.sqrt([self.n_in, self.n_h])\n",
    "        up_bound = 1 / np.sqrt([self.n_in, self.n_h])\n",
    "        \n",
    "        # Initialize the parameters\n",
    "        np.random.seed(123)\n",
    "        self.W1 = np.random.uniform(low_bound[0], up_bound[0], size=(self.n_h, self.n_in))  # d_h x d\n",
    "        self.W2 = np.random.uniform(low_bound[1], up_bound[1], size=(self.n_o, self.n_h))  # m x d_h\n",
    "        self.b1 = np.zeros(self.n_h)  # dimension d_h\n",
    "        self.b2 = np.zeros(self.n_o) # dimension m\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        '''\n",
    "        Computes activations for every layer\n",
    "        X: input data set\n",
    "        '''\n",
    "        self.ha = self.W1.dot(X.T) + self.b1.reshape(self.n_h, 1)\n",
    "        self.hs = relu(self.ha)\n",
    "        self.oa = self.W2.dot(self.hs) + self.b2.reshape(self.n_o, 1)\n",
    "        self.os = np.transpose(softmax(self.oa.T))\n",
    "            \n",
    "    def bprop(self, X, Y):\n",
    "        '''\n",
    "        Computes the gradients, must be executed after fprop\n",
    "        X: Input data set\n",
    "        Y: targets\n",
    "        '''\n",
    "        n = X.shape[0]\n",
    "                      \n",
    "        grad_oa = self.os - onehot_matrix(self.n_o, Y)\n",
    "        grad_b2 = np.sum(grad_oa, axis =1) # m x n\n",
    "        grad_W2 = np.dot(grad_oa, self.hs.T) # sum of gradients grad_W2 for each example\n",
    "        grad_hs = self.W2.T.dot(grad_oa) # d_h x n\n",
    "        grad_ha = grad_hs * (self.ha > 0) # d_h x n\n",
    "        grad_W1 = np.dot(grad_ha, X) # sum of gradients grad_W1 for each example\n",
    "        grad_b1 = np.sum(grad_ha, axis =1) # d_h x n\n",
    "        \n",
    "        return grad_W1 / n, grad_W2 / n, grad_b1 / n, grad_b2 / n #returns average of the gradient\n",
    "    \n",
    "    def compute_loss(self, Y):\n",
    "        ind = [i for i in range(Y.shape[0])]\n",
    "        loss = self.os[Y.astype(int),ind]\n",
    "        return np.mean(-np.log(loss))\n",
    "    \n",
    "    def train(self, train_data, valid_data, test_data, max_iter, eta=0.05):\n",
    "        \n",
    "        n_batches = int(np.ceil(train_data.shape[0]/self.K)) # number of batches\n",
    "        \n",
    "        for i in range(max_iter):\n",
    "            for j in range(0,train_data.shape[0],self.K):\n",
    "                batch = train_data[j:j+self.K]\n",
    "                \n",
    "                self.fprop(batch[:,:-1])\n",
    "                grad_W1_mean, grad_W2_mean, grad_b1_mean, grad_b2_mean = self.bprop(batch[:,:-1], batch[:,-1]) \n",
    "                \n",
    "                n = len(batch)\n",
    "                \n",
    "                #regularization\n",
    "                penality_grad_W1 = self.lambdas[0][0] * np.sign(self.W1) + 2 * self.lambdas[0][1] * self.W1\n",
    "                penality_grad_W2 = self.lambdas[1][0] * np.sign(self.W2) + 2 * self.lambdas[1][1] * self.W2\n",
    "                \n",
    "                self.W1 = self.W1 - eta * (grad_W1_mean + penality_grad_W1)\n",
    "                self.W2 = self.W2 - eta * (grad_W2_mean + penality_grad_W2)\n",
    "                self.b1 = self.b1 - eta * grad_b1_mean\n",
    "                self.b2 = self.b2 - eta * grad_b2_mean\n",
    "                    \n",
    "            self.write_in_file(i,train_data, valid_data, test_data)\n",
    "                    \n",
    "    def compute_predictions(self, test_data):\n",
    "        self.fprop(test_data)\n",
    "        pred = self.os\n",
    "        return pred\n",
    "    \n",
    "    def write_in_file(self,epoque,train_data,valid_data,test_data):\n",
    "        train_prob = self.compute_predictions(train_data[:,:-1])\n",
    "        train_classe_pred = np.argmax(train_prob, axis = 0)  \n",
    "        valid_prob = self.compute_predictions(valid_data[:,:-1])\n",
    "        valid_classe_pred = np.argmax(valid_prob, axis = 0)\n",
    "        test_prob = self.compute_predictions(test_data[:,:-1])\n",
    "        test_classe_pred = np.argmax(test_prob, axis = 0)\n",
    "        classif_erreur_train = 100*float(sum(sum([train_classe_pred != train_data[:,-1]])))/train_data.shape[0]\n",
    "        classif_erreur_valid = 100*float(sum(sum([valid_classe_pred != valid_data[:,-1]])))/valid_data.shape[0]\n",
    "        classif_erreur_test = 100*float(sum(sum([test_classe_pred != test_data[:,-1]])))/test_data.shape[0]\n",
    "        self.fprop(train_data[:,:-1])\n",
    "        loss_train = self.compute_loss(train_data[:,-1])\n",
    "        self.fprop(valid_data[:,:-1])\n",
    "        loss_valid = self.compute_loss(valid_data[:,-1])\n",
    "        self.fprop(test_data[:,:-1])\n",
    "        loss_test = self.compute_loss(test_data[:,-1])\n",
    "        \n",
    "        # Enregistre dans le fichier les résultats a chaque epoque.\n",
    "        # file : les résultat sont écrit de facon compréhensible\n",
    "        # fileforgraph pour realiser les graphiques\n",
    "        if epoque==0: # Ecrase le fichier s'il existe\n",
    "            file=open(\"Question_9.txt\",\"w\")\n",
    "            file_forgraph=open(\"Question_9_forgraph.txt\",\"w\")\n",
    "        else: # ecrit en bas du fichier\n",
    "            file = open(\"Question_9.txt\", \"a\")\n",
    "            file_forgraph=open(\"Question_9_forgraph.txt\",\"a\")\n",
    "        toprint= 'Epoque %d,ERREUR - train : %f, valid : %f, test :%f,\\\n",
    "        COUT MOYEN train : %f, valid : %f, test: %f'\\\n",
    "        % (epoque+1,classif_erreur_train,classif_erreur_valid,classif_erreur_test,\\\n",
    "        loss_train, loss_valid, loss_test)\n",
    "        \n",
    "        toprint_forgraph  = '%d %f %f %f %f %f %f' % (epoque+1,classif_erreur_train,classif_erreur_valid,\n",
    "                                                      classif_erreur_test,loss_train, loss_valid, loss_test)\n",
    "        # Ecriture dans les fichiers\n",
    "        print >> file, toprint\n",
    "        print >> file_forgraph, toprint_forgraph\n",
    "        \n",
    "        file.close()\n",
    "        file_forgraph.close()\n",
    "        #Afficher les résultats pour la recherche des meilleur parametre\n",
    "        print 'Epoque : %d Erreur sur le valid : %f' % (epoque+1,classif_erreur_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 3 7 2 2 6 7 0 0 9 7 3 8 0 6 8 8 8 8 3]\n"
     ]
    }
   ],
   "source": [
    "lambdas = np.array([[0.01, 0.01],\n",
    "           [0.01, 0.01]])\n",
    "nn_mnist = NeuralNetVectorized2(784,64,10,lambdas,train.shape[0])\n",
    "\n",
    "nn_mnist.fprop(train[:,:-1])\n",
    "#print(nn_mnist.ha)\n",
    "#print(nn_mnist.hs)\n",
    "#print(nn_mnist.oa)\n",
    "#print(nn_mnist.os)\n",
    "print(nn_mnist.compute_loss(train[:,-1]))\n",
    "nn_mnist.train(train,test,valid,10,0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lambdas = np.array([[0.0, 0.0],\n",
    "           [0.00, 0.00]])\n",
    "nn_mnist = NeuralNetVectorized2(784,256,10,lambdas,10)\n",
    "nn_mnist.train(train, valid, test, 20,eta=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Graphique de l'erreur de classification et du cout sur mnist a partir des \n",
    "import matplotlib.pyplot as plt\n",
    "data=np.loadtxt(open('Question_9_forgraph.txt','r'))\n",
    "epoque=data[:,0]\n",
    "erreur_train=data[:,1]\n",
    "erreur_valid=data[:,2]\n",
    "erreur_test=data[:,3]\n",
    "cout_train=data[:,4]\n",
    "cout_valid=data[:,5]\n",
    "cout_test=data[:,6]\n",
    "plt.plot(epoque, erreur_train,label='train')\n",
    "plt.plot(epoque, erreur_valid, label='valid')\n",
    "plt.plot(epoque, erreur_test,label='test')\n",
    "plt.legend()\n",
    "plt.xlabel('epoque')\n",
    "plt.ylabel('erreur en %')\n",
    "plt.title('Taux d erreur de Classification')\n",
    "plt.show()\n",
    "plt.plot(epoque, cout_train,label='train')\n",
    "plt.plot(epoque, cout_valid, label='valid')\n",
    "plt.plot(epoque, cout_test,label='test')\n",
    "plt.legend()\n",
    "plt.xlabel('epoque')\n",
    "plt.ylabel('cout')\n",
    "plt.title('Cout')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
