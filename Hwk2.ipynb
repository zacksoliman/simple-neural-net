{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation du reseau de neurones et experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul numÃ©riquement stable du softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_vector(x):\n",
    "    '''\n",
    "    x:  is a data vector.\n",
    "    returns: the result of the softmax function applied to the data vector.\n",
    "    '''\n",
    "    max_comp = np.amax(x)\n",
    "    normalized  = x - max_comp\n",
    "    \n",
    "    exponential = np.exp(normalized)\n",
    "    \n",
    "    return exponential/np.sum(exponential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    '''\n",
    "    X: matrix that holds the data, every row is a data vector.\n",
    "    returns: matrix where every row is the result of the softmax function applied to the corresponding data vector.\n",
    "    '''\n",
    "    \n",
    "    max_comp = np.amax(X, axis=1)\n",
    "    normalized  = X - max_comp.reshape(X.shape[0], 1)\n",
    "    \n",
    "    exponential = np.exp(normalized)\n",
    "    \n",
    "    return exponential/np.sum(exponential, axis=1).reshape(X.shape[0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction utilitaire pour calculer relu($x$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x, np.zeros(x.shape))\n",
    "\n",
    "def onehot(m, y):\n",
    "    return np.eye(m)[y]\n",
    "\n",
    "def onehot_matrix(m, targets):\n",
    "    \"\"\"\n",
    "    Returns: onehot matrix where every column is a onehot vector of the coressponding target\n",
    "    \"\"\"\n",
    "    eye = np.eye(m)\n",
    "    onehot_matrix = np.zeros((m,len(targets)))\n",
    "    \n",
    "    for i, y in enumerate(targets):\n",
    "        onehot_matrix[:,i] = eye[y]\n",
    "        \n",
    "    return onehot_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 et quesiton 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation de fprop et bprop pour calculer le gradient sur un exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    \n",
    "    def __init__(self, n_input, n_hidden, n_out):\n",
    "        \n",
    "        self.n_in = n_input\n",
    "        self.n_h = n_hidden\n",
    "        self.n_o = n_out\n",
    "        \n",
    "        low_bound = -1 / np.sqrt([self.n_in, self.n_h])\n",
    "        up_bound = 1 / np.sqrt([self.n_in, self.n_h])\n",
    "        \n",
    "        # Initialize the parameters\n",
    "        self.W1 = np.random.uniform(low_bound[0], up_bound[0], size=(self.n_h, self.n_in))  # d_h x d\n",
    "        self.W2 = np.random.uniform(low_bound[1], up_bound[1], size=(self.n_o, self.n_h))  # m x d_h\n",
    "        self.b1 = np.zeros(self.n_h)  # dimension d_h\n",
    "        self.b2 = np.zeros(self.n_o) # dimension m\n",
    "    \n",
    "    def fprop(self, x):\n",
    "        '''Computes activations for every layer'''\n",
    "        self.ha = self.W1.dot(x) + self.b1\n",
    "        self.hs = relu(ha)\n",
    "        self.oa = self.W2.dot(hs) + self.b2\n",
    "        self.os = softmax_vector(oa)\n",
    "            \n",
    "    def bprop(self, x, y):\n",
    "        '''Computes the gradients, must be executed after fprop'''\n",
    "                      \n",
    "        grad_oa = os - onehot(self.n_out, y)\n",
    "        grad_b2 = grad_oa\n",
    "        grad_W2 = np.outer(grad_oa, self.hs)\n",
    "        grad_hs = self.W2.T.dot(grad_oa)\n",
    "        grad_ha = grad_hs * (self.ha > 0)\n",
    "        grad_W1 = np.outer(grad_ha, x)\n",
    "        grad_b1 = grad_ha\n",
    "        \n",
    "        return grad_W1, grad_W2, grad_b1, grad_b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction pour calculer les differences finies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(x, y, W1, W2, b1, b2):\n",
    "    ha = W1.dot(x) + b1\n",
    "    hs = relu(ha)\n",
    "    oa = W2.dot(hs) + b2\n",
    "    os = softmax_vector(oa)\n",
    "    \n",
    "    return -np.log(os[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def finite_diff(x, neural_net, eps=1e-5):\n",
    "    \n",
    "    # params\n",
    "    #W1 = neural_net.W1\n",
    "    #w2 = neural_net.W2\n",
    "    #b1 = neural_net.b1\n",
    "    #b2 = neural_net.b2\n",
    "    \n",
    "    #neural_net.fprop(x)\n",
    "    #loss = -np.log(neural_net.os)\n",
    "    \n",
    "    #W1 = W1 - eps\n",
    "    #W2 = W2 - eps\n",
    "    #b1 = b1 - eps\n",
    "    #b2 = b2 - eps\n",
    "    \n",
    "    #new_loss = (x, W1, W2, b1, b2)\n",
    "    \n",
    "    #W1 = W1 + eps\n",
    "    #W2 = W2 + eps\n",
    "    #b1 = b1 + eps\n",
    "    #b2 = b2 + eps\n",
    "    \n",
    "    #return (new_loss - loss)/eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt(open('2moons.txt','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = NeuralNet(2, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 et 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    \n",
    "    def __init__(self, n_input, n_hidden, n_out, lambdas):\n",
    "        \n",
    "        self.n_in = n_input\n",
    "        self.n_h = n_hidden\n",
    "        self.n_o = n_out\n",
    "        self.lambdas = lambdas\n",
    "        \n",
    "        low_bound = -1 / np.sqrt([self.n_in, self.n_h])\n",
    "        up_bound = 1 / np.sqrt([self.n_in, self.n_h])\n",
    "        \n",
    "        # Initialize the parameters\n",
    "        self.W1 = np.random.uniform(low_bound[0], up_bound[0], size=(self.n_h, self.n_in))  # d_h x d\n",
    "        self.W2 = np.random.uniform(low_bound[1], up_bound[1], size=(self.n_o, self.n_h))  # m x d_h\n",
    "        self.b1 = np.zeros(self.n_h)  # dimension d_h\n",
    "        self.b2 = np.zeros(self.n_o) # dimension m\n",
    "    \n",
    "    def fprop(self, x):\n",
    "        '''Computes activations for every layer'''\n",
    "        self.ha = self.W1.dot(x) + self.b1\n",
    "        self.hs = relu(ha)\n",
    "        self.oa = self.W2.dot(hs) + self.b2\n",
    "        self.os = softmax_vector(oa)\n",
    "            \n",
    "    def bprop(self, x, y):\n",
    "        '''Computes the gradients, must be executed after fprop'''\n",
    "                      \n",
    "        grad_oa = os - onehot(self.n_out, y)\n",
    "        grad_b2 = grad_oa\n",
    "        grad_W2 = np.outer(grad_oa, self.hs)\n",
    "        grad_hs = self.W2.T.dot(grad_oa)\n",
    "        grad_ha = grad_hs * (self.ha > 0)\n",
    "        grad_W1 = np.outer(grad_ha, x)\n",
    "        grad_b1 = grad_ha\n",
    "        \n",
    "        return grad_W1, grad_W2, grad_b1, grad_b2\n",
    "    \n",
    "    def compute_loss(self, y):\n",
    "        return -np.log(self.os[y])\n",
    "    \n",
    "    def train(self, train_data, max_iter, batch_size, eta=0.05):\n",
    "        \n",
    "        n_batches = np.ceil(train_data.shape[0]/batch_size) # number of batches\n",
    "        \n",
    "        # Initialize batch start and end indices\n",
    "        batch_start = 0\n",
    "        if (batch_start + batch_size < train_data.shape[0]):\n",
    "            batch_end = batch_start + batch_size\n",
    "        else:\n",
    "            batch_end = train_data.shape[0]\n",
    "        \n",
    "        for i in range(max_iter):\n",
    "            for j in range(n_batches):\n",
    "                \n",
    "                batch = train_data[batch_start:batch_end]\n",
    "                grad_W1_mean = np.zeros((self.n_h, self.n_in))\n",
    "                grad_W2_mean = np.zeros((self.n_o, self.n_h))\n",
    "                grad_b1_mean = np.zeros(self.n_h)\n",
    "                grad_b2_mean = np.zeros(self.n_o)\n",
    "                \n",
    "                for elem in batch:\n",
    "                    fprop(elem[-1])                    \n",
    "                    losses.push(compute_loss(elem[-1]))\n",
    "                    \n",
    "                    grad_W1, grad_W2, grad_b1, grad_b2 = bprop(elem[:-1], elem[-1])\n",
    "                    \n",
    "                    grad_w1_mean = grad_W1_mean + grad_W1\n",
    "                    grad_w2_mean = grad_W2_mean + grad_W2\n",
    "                    grad_b1_mean = grad_b1_mean + grad_b1\n",
    "                    grad_b2_mean = grad_b2_mean + grad_b2\n",
    "                \n",
    "                # Compute mean of loss/cost over all examples in the batch\n",
    "                n = len(batch)\n",
    "                grad_w1_mean = grad_W1_mean / n\n",
    "                grad_w2_mean = grad_W2_mean / n\n",
    "                grad_b1_mean = grad_b1_mean / n\n",
    "                grad_b2_mean = grad_b2_mean / n\n",
    "                \n",
    "                #regularization\n",
    "                penality_grad_W1 = self.lambdas[0][0] * np.sign(self.W1) + 2 * self.lambdas[0][1] * self.W1\n",
    "                penality_grad_W2 = self.lambdas[1][0] * np.sign(self.W2) + 2 * self.lambdas[1][1] * self.W2\n",
    "                \n",
    "                self.W1 = self.W1 - eta * (grad_W1_mean + penality_grad_W1)\n",
    "                self.W2 = self.W2 - eta * (grad_W2_mean + penality_grad_W2)\n",
    "                self.b1 = self.b1 - eta * grad_b1_mean\n",
    "                self.b2 = self.b2 - eta * grad_b2_mean\n",
    "                \n",
    "                # Get next batch\n",
    "                batch_start = batch_end + 1        \n",
    "                if (batch_start + batch_size < train_data.shape[0]):\n",
    "                    batch_end = batch_start + batch_size\n",
    "                else:\n",
    "                    batch_end = train_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Entrainement sur 2 moons et visualisation des regions de decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: Optimisation du calcule de gradient pour mini-batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{W}^{(1)} \\in \\mathbb{R}^{d_h \\times d}$, $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ et $\\mathbf{B}^{(1)} \\in \\mathbb{R}^{d_h \\times n}$\n",
    "\n",
    "$$\\mathbf{h}^{a} = \\mathbf{W}^{(1)}\\mathbf{X}^{\\top} + \\mathbf{B}^{(1)} \\in \\mathbb{R}^{d_h \\times n}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetVectorized:\n",
    "    \n",
    "    def __init__(self, n_input, n_hidden, n_out, lambdas):\n",
    "        \n",
    "        self.n_in = n_input\n",
    "        self.n_h = n_hidden\n",
    "        self.n_o = n_out\n",
    "        self.lambdas = lambdas\n",
    "        \n",
    "        low_bound = -1 / np.sqrt([self.n_in, self.n_h])\n",
    "        up_bound = 1 / np.sqrt([self.n_in, self.n_h])\n",
    "        \n",
    "        # Initialize the parameters\n",
    "        self.W1 = np.random.uniform(low_bound[0], up_bound[0], size=(self.n_h, self.n_in))  # d_h x d\n",
    "        self.W2 = np.random.uniform(low_bound[1], up_bound[1], size=(self.n_o, self.n_h))  # m x d_h\n",
    "        self.b1 = np.zeros(self.n_h)  # dimension d_h\n",
    "        self.b2 = np.zeros(self.n_o) # dimension m\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        '''\n",
    "        Computes activations for every layer\n",
    "        X: input data set\n",
    "        '''\n",
    "        self.ha = self.W1.dot(X.T) + self.b1.reshape(self.n_h, 1)\n",
    "        self.hs = relu(ha)\n",
    "        self.oa = self.W2.dot(hs) + self.b2.reshape(self.n_o, 1)\n",
    "        self.os = softmax(oa)\n",
    "            \n",
    "    def bprop(self, X, Y):\n",
    "        '''\n",
    "        Computes the gradients, must be executed after fprop\n",
    "        X: Input data set\n",
    "        Y: targets\n",
    "        '''\n",
    "                      \n",
    "        grad_oa = os - onehot_matrix(self.n_out, Y)\n",
    "        grad_b2 = grad_oa # m x n\n",
    "        grad_W2 = np.dot(grad_oa, self.hs.T) # sum of gradients grad_W2 for each example\n",
    "        grad_hs = self.W2.T.dot(grad_oa) # d_h x n\n",
    "        grad_ha = grad_hs * (self.ha > 0) # d_h x n\n",
    "        grad_W1 = np.dot(grad_ha, X.T) # sum of gradients grad_W1 for each example\n",
    "        grad_b1 = grad_ha # d_h x n\n",
    "        \n",
    "        return grad_W1, grad_W2, grad_b1, grad_b2\n",
    "    \n",
    "    def compute_loss(self, y):\n",
    "        # TODO: adjust for y vector\n",
    "        return -np.log(self.os)\n",
    "    \n",
    "    def train(self, train_data, max_iter, batch_size, eta=0.05):\n",
    "        \n",
    "        n_batches = np.ceil(train_data.shape[0]/batch_size) # number of batches\n",
    "        \n",
    "        # Initialize batch start and end indices\n",
    "        batch_start = 0\n",
    "        if (batch_start + batch_size < train_data.shape[0]):\n",
    "            batch_end = batch_start + batch_size\n",
    "        else:\n",
    "            batch_end = train_data.shape[0]\n",
    "        \n",
    "        for i in range(max_iter):\n",
    "            for j in range(n_batches):\n",
    "                \n",
    "                batch = train_data[batch_start:batch_end]\n",
    "                \n",
    "                fprop(batch[:,:-1])\n",
    "                grad_W1, grad_W2, grad_b1, grad_b2 = bprop(batch[:,:-1], batch[:,-1]) \n",
    "                \n",
    "                n = len(batch)\n",
    "                \n",
    "                #regularization\n",
    "                penality_grad_W1 = self.lambdas[0][0] * np.sign(self.W1) + 2 * self.lambdas[0][1] * self.W1\n",
    "                penality_grad_W2 = self.lambdas[1][0] * np.sign(self.W2) + 2 * self.lambdas[1][1] * self.W2\n",
    "                \n",
    "                self.W1 = self.W1 - eta * ((grad_W1 / n) + penality_grad_W1)\n",
    "                self.W2 = self.W2 - eta * ((grad_W2 / n) + penality_grad_W2)\n",
    "                self.b1 = self.b1 - eta * np.mean(grad_b1, axis=1)\n",
    "                self.b2 = self.b2 - eta * np.mean(grad_b2, axis=1)\n",
    "                \n",
    "                \n",
    "                # Get next batch\n",
    "                batch_start = batch_end + 1        \n",
    "                if (batch_start + batch_size < train_data.shape[0]):\n",
    "                    batch_end = batch_start + batch_size\n",
    "                else:\n",
    "                    batch_end = train_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
